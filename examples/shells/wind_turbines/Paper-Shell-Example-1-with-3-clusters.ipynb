{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The libraries we will use\n",
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "\n",
    "# importing LUQ\n",
    "from luq.luq import *\n",
    "\n",
    "# distributions for data-generating samples and comparing approx vs true solutions\n",
    "from scipy.stats import norm, beta\n",
    "\n",
    "# Gaussian KDE \n",
    "from scipy.stats import gaussian_kde as GKDE\n",
    "\n",
    "# quadrautre for TV metric\n",
    "from scipy.integrate import quadrature\n",
    "\n",
    "# plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.tri as tri\n",
    "import ipywidgets as wd\n",
    "\n",
    "# colorblind friendly color palette\n",
    "c = ['#377eb8', '#ff7f00', '#4daf4a',\n",
    "                  '#f781bf', '#a65628', '#984ea3',\n",
    "                  '#999999', '#e41a1c', '#dede00']\n",
    "\n",
    "# Set up fontsizes for plots\n",
    "plt_params = {'legend.fontsize': 14,\n",
    "          'figure.figsize': (6.4, 4.8),\n",
    "         'axes.labelsize': 16,\n",
    "         'axes.titlesize': 16,\n",
    "         'xtick.labelsize': 14,\n",
    "         'ytick.labelsize': 14}\n",
    "plt.rcParams.update(plt_params)\n",
    "\n",
    "np.random.seed(123456)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %matplotlib widget"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the initial dataset containing both model data (used to generate predicted data) and parameter samples from initial distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!pip install mat73  # For reading in Matlab 7.3 files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import mat73 as mat73"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "init_data_set = mat73.loadmat('../SteelDrums/1DCase1-Feb22-2023/1DCase1/Prior/prior.mat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "init_data_set.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "xs = init_data_set['xs']\n",
    "ys = init_data_set['ys']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_init = ys\n",
    "\n",
    "a0_init = init_data_set['a0']  # initial samples of first parameter\n",
    "a1_init = init_data_set['a1']  # initial samples of second parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now load/analyze the data-generating dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "obs_data_set = mat73.loadmat('../SteelDrums/1DCase1-Feb22-2023/1DCase1/Observed/observed.mat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "obs_data_set.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_obs = obs_data_set['ys']\n",
    "\n",
    "a0_obs = obs_data_set['a0']  # samples of first parameter responsible for observed data\n",
    "a1_obs = obs_data_set['a1']  # samples of second parameter responsible for observed data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What if we used all sensor data (with NO noise), built a classifier from *known* labels of the initial data to determine whether observed data belonged to elliptic or hyperbolic parameter types, and then pieced together a global inverse solution from local inverse solutions?\n",
    "\n",
    "This is the most ideal scenario to determine the best possible case for our inverse solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# First get known labels from initial dataset\n",
    "\n",
    "idx_mild_deformation = np.where((a1_init>-0.075) & (a1_init<0.075))[0]\n",
    "idx_more_hyperbolic = np.where(a1_init>0.075)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(idx_mild_deformation.size)\n",
    "print(idx_more_hyperbolic.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "init_labels = np.zeros(len(a1_init))\n",
    "init_labels[idx_mild_deformation] = 1\n",
    "init_labels[idx_more_hyperbolic] = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the observable data function that transforms\n",
    "# the model data into data values at sensor points\n",
    "\n",
    "def generate_observable_data(data, xs, n_sensors):\n",
    "    x_sensors = np.linspace(xs[0], xs[-1], n_sensors)\n",
    "    return np.interp(x_sensors, xs, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "n_sensors = int(1001)\n",
    "\n",
    "x_sensors = np.linspace(xs[0], xs[-1], n_sensors)\n",
    "\n",
    "num_init = int(1e4)\n",
    "\n",
    "data_init_sensors = np.zeros((num_init, n_sensors))\n",
    "for i in range(num_init):\n",
    "    data_init_sensors[i,:] = generate_observable_data(data_init[i,:], xs, n_sensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_obs = int(3e3)\n",
    "\n",
    "data_obs_sensors = np.zeros((num_obs, n_sensors))\n",
    "for i in range(num_obs):\n",
    "    data_obs_sensors[i,:] = generate_observable_data(data_obs[i,:], xs, n_sensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "xs_include = list(np.arange(50,n_sensors-50))\n",
    "\n",
    "learn = LUQ(predicted_data = data_init_sensors[:,xs_include],\n",
    "            observed_data = data_obs_sensors[:,xs_include])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This takes the longest to run because of the size of the datasets.\n",
    "# Expect to wait a couple of minutes.\n",
    "# One could use a higher C value, but this will take longer to run and did not result \n",
    "# in better results than those obtained with C=1e2. \n",
    "\n",
    "learn.dynamics(custom_labels=init_labels,\n",
    "               proposals=({'kernel': 'linear', 'C': 1e1},\n",
    "                          {'kernel': 'linear', 'C': 1e2}),\n",
    "               relabel_predictions=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pred_maps, obs_maps = learn.learn_qois_and_transform(num_qoi=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Generate kernel density estimates on new QoI and calculate new weights\n",
    "pi_predict_kdes = []\n",
    "pi_obs_kdes = []\n",
    "r_vals = []\n",
    "r_means = []\n",
    "for i in range(learn.num_clusters):\n",
    "    pi_predict_kdes.append(GKDE(learn.predict_maps[i].T))\n",
    "    pi_obs_kdes.append(GKDE(learn.obs_maps[i].T))\n",
    "    r_vals.append(\n",
    "                np.divide(\n",
    "                    pi_obs_kdes[i](\n",
    "                    learn.predict_maps[i].T), \n",
    "                    pi_predict_kdes[i](\n",
    "                    learn.predict_maps[i].T)))\n",
    "    r_means.append(np.mean(r_vals[i]))\n",
    "print(f'Diagnostics: {r_means}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "params = np.vstack((a0_init, a1_init)).T\n",
    "\n",
    "params_obs = np.vstack((a0_obs, a1_obs)).T\n",
    "\n",
    "param_marginals = []\n",
    "true_param_marginals = []\n",
    "lam_ptr = []\n",
    "cluster_weights = []\n",
    "param_marginals_modified = []\n",
    "modified_r_values = np.zeros(len(a0_init))\n",
    "\n",
    "for i in range(learn.num_clusters):\n",
    "    lam_ptr.append(np.where(learn.predict_labels == i)[0])\n",
    "    cluster_weights.append(len(np.where(learn.obs_labels == i)[0]) / num_obs)\n",
    "    modified_r_values[lam_ptr[i]] = r_vals[i]*cluster_weights[i]  \n",
    "    \n",
    "for i in range(params.shape[1]):\n",
    "    true_param_marginals.append(GKDE(params_obs[:,i]))\n",
    "    param_marginals_modified.append(GKDE(params[:,i], weights=modified_r_values))\n",
    "    param_marginals.append([])\n",
    "    for k in range(learn.num_clusters):\n",
    "        param_marginals[i].append(GKDE(params[lam_ptr[k], i], weights=r_vals[k]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(cluster_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weighted_diagnostic = 0\n",
    "for i in range(learn.num_clusters):\n",
    "    weighted_diagnostic += cluster_weights[i] * r_means[i]\n",
    "print(weighted_diagnostic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "param_labels = ['$a_0$', '$a_1$']\n",
    "\n",
    "for i in range(params.shape[1]):\n",
    "    if i==0:\n",
    "        x = np.linspace(0.8,1.2,100)\n",
    "    else:\n",
    "        x = np.linspace(-0.2,0.2,100)\n",
    "        \n",
    "    fig = plt.figure()\n",
    "    fig.clear()\n",
    "    \n",
    "    plt.plot(x, true_param_marginals[i](x), label = 'KDE of DG; full set', \n",
    "             linewidth=2, linestyle='dashed')\n",
    "    \n",
    "    plt.plot(x,1/0.4*np.ones(len(x)), linewidth=1)\n",
    "\n",
    "    plt.plot(x, param_marginals_modified[i](x), \n",
    "             label = r'Update; full set', linewidth=2, linestyle='dashdot')\n",
    "    \n",
    "    if i==1:\n",
    "        plt.axvline(0, color='k', lw=2, ls='-.')\n",
    "    plt.title('Densities for parameter ' + param_labels[i])\n",
    "    plt.legend()\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now add noise to observations, use less initial and observed samples, use LUQ to filter based on known labels of reduced initial sample set, classify, and learn QoI.\n",
    "\n",
    "This will produce the operational case studies for limited datasets that better reflect real-world conditions that are then compared to the ideal case above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_init = int(2E2)\n",
    "\n",
    "data_init_subset_sensors = np.zeros((num_init, n_sensors))\n",
    "for i in range(num_init):\n",
    "    data_init_subset_sensors[i,:] = generate_observable_data(data_init[i,:], xs, n_sensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Generate 3 different observation datasets\n",
    "\n",
    "num_obs = int(7.5e1)\n",
    "\n",
    "start_idx = [750, 1500, 2250]\n",
    "\n",
    "num_sets = len(start_idx)\n",
    "\n",
    "data_obs_subset_sensors = np.zeros((num_sets, num_obs, n_sensors))\n",
    "for j in range(num_sets):\n",
    "    for i in range(num_obs):\n",
    "        data_obs_subset_sensors[j,i,:] = generate_observable_data(data_obs[start_idx[j]+i,:], xs, n_sensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "SNR_obs = 5\n",
    "\n",
    "var_noise = np.var(data_init_subset_sensors, axis=0) / SNR_obs\n",
    "\n",
    "noisy_obs_subset_sensors = data_obs_subset_sensors +\\\n",
    "                            np.random.randn(\n",
    "                                np.shape(data_obs_subset_sensors)[0], \n",
    "                                np.shape(data_obs_subset_sensors)[1],\n",
    "                                np.shape(data_obs_subset_sensors)[2]) * np.sqrt(var_noise)\n",
    "\n",
    "SNR_init = 10\n",
    "\n",
    "var_noise = np.var(data_init_subset_sensors, axis=0) / SNR_init\n",
    "\n",
    "noisy_data_init_subset_sensors = data_init_subset_sensors +\\\n",
    "            np.random.randn(np.shape(data_init_subset_sensors)[0], \n",
    "                            np.shape(data_init_subset_sensors)[1]) * np.sqrt(var_noise)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "learn_base = LUQ(noisy_data_init_subset_sensors[:num_init, xs_include])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_filtered_obs = 60  # Using this many filtered data to learn QoI \n",
    "\n",
    "filtered_data_coordinates = np.linspace(x_sensors[xs_include[0]],\n",
    "                                        x_sensors[xs_include[-1]],\n",
    "                                        num_filtered_obs)\n",
    "\n",
    "predicted_data_coordinates = x_sensors[xs_include]\n",
    "\n",
    "learn_base.filter_data(filter_method='splines',\n",
    "                       tol=1e-2, \n",
    "                       min_knots=6, \n",
    "                       max_knots=10,\n",
    "                       filtered_data_coordinates = filtered_data_coordinates, \n",
    "                       predicted_data_coordinates = predicted_data_coordinates\n",
    "                       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cluster_labels = np.zeros(num_init, dtype=int)\n",
    "\n",
    "idx_mild_deformation = np.where((a1_init[:num_init]>-0.075) & (a1_init[:num_init]<0.075))[0]\n",
    "idx_more_hyperbolic = np.where(a1_init[:num_init]>0.075)[0]\n",
    "\n",
    "cluster_labels[idx_mild_deformation] = 1\n",
    "cluster_labels[idx_more_hyperbolic] = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(idx_mild_deformation.size)\n",
    "print(idx_more_hyperbolic.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "learn_base.dynamics(custom_labels=cluster_labels,\n",
    "                   proposals=({'kernel': 'linear', 'C': 1e3},\n",
    "                              {'kernel': 'linear', 'C': 1e4},\n",
    "                              {'kernel': 'linear', 'C': 1e5},\n",
    "                              {'kernel': 'linear', 'C': 1e6}),\n",
    "                   relabel_predictions=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "learn_base.learn_qois_and_transform(num_qoi=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare to 30 filtered observations for sufficiency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "learn_base_test = LUQ(noisy_data_init_subset_sensors[:num_init, xs_include])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_filtered_obs_test = 30  # Using this many filtered data to learn QoI \n",
    "\n",
    "filtered_data_coordinates = np.linspace(x_sensors[xs_include[0]],\n",
    "                                        x_sensors[xs_include[-1]],\n",
    "                                        num_filtered_obs_test)\n",
    "\n",
    "predicted_data_coordinates = x_sensors[xs_include]\n",
    "\n",
    "learn_base_test.filter_data(filter_method='splines',\n",
    "                       tol=1e-2, \n",
    "                       min_knots=6, \n",
    "                       max_knots=10,\n",
    "                       filtered_data_coordinates = filtered_data_coordinates, \n",
    "                       predicted_data_coordinates = predicted_data_coordinates\n",
    "                       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "learn_base_test.dynamics(custom_labels=cluster_labels,\n",
    "                   proposals=({'kernel': 'linear', 'C': 1e3},\n",
    "                              {'kernel': 'linear', 'C': 1e4},\n",
    "                              {'kernel': 'linear', 'C': 1e5},\n",
    "                              {'kernel': 'linear', 'C': 1e6}),\n",
    "                   relabel_predictions=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "learn_base_test.learn_qois_and_transform(num_qoi=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LUQs = [learn_base_test, learn_base]\n",
    "\n",
    "# normalize eigenvectors\n",
    "unit_alphas = []\n",
    "for i in range(2):  # Loop through each of the filtering data options\n",
    "    unit_alphas.append([])\n",
    "    for k in range(3):  # Loop through each of the clusters\n",
    "        unit_alphas[i].append([])\n",
    "        for j in range(2):\n",
    "            unit_alphas[i][k].append(LUQs[i].kpcas[k].eigenvectors_[:,j] / np.linalg.norm(LUQs[i].kpcas[k].eigenvectors_[:,j], ord=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# applying linear regression for each grid pair\n",
    "from scipy.linalg import lstsq\n",
    "\n",
    "As = []\n",
    "ms = []\n",
    "bs = []\n",
    "R_squared = []\n",
    "for i in range(1):\n",
    "    As.append([])\n",
    "    ms.append([])\n",
    "    bs.append([])\n",
    "    R_squared.append([])\n",
    "    for k in range(3):  # Loop through each of the clusters \n",
    "        As[i].append([])\n",
    "        ms[i].append([])\n",
    "        bs[i].append([])\n",
    "        R_squared[i].append([])\n",
    "        for j in range(2):\n",
    "            As[i][k].append(np.ones((len(unit_alphas[i][k][j]),2)))\n",
    "            As[i][k][j][:,1] = unit_alphas[-(i+1)][k][j]\n",
    "            coeffs, res, _, _ = lstsq(As[i][k][j], unit_alphas[-(i+2)][k][j])\n",
    "            ms[i][k].append(coeffs[1])\n",
    "            bs[i][k].append(coeffs[0])\n",
    "            SS_tot = np.sum((unit_alphas[-(i+1)][k][j] - np.mean(unit_alphas[-(i+2)][k][j]))**2)\n",
    "            R_squared[i][k].append(1-res/SS_tot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ms)\n",
    "print('~'*100)\n",
    "print(R_squared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "learn_base.save_instance('luq_base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "learn_list = []\n",
    "for j in range(num_sets):\n",
    "    LUQ_base_file = open(\"luq_base\", \"rb\")\n",
    "    learn_list.append(pickle.load(LUQ_base_file))\n",
    "    LUQ_base_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for j in range(num_sets):\n",
    "    learn_list[j].set_observations(noisy_obs_subset_sensors[j, :num_obs, xs_include].T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "\n",
    "plt.scatter(x_sensors[xs_include], noisy_data_init_subset_sensors[0,xs_include].T, \n",
    "            s=5, marker='s', c='b', alpha=0.25, label='Noisy Predicted Data')\n",
    "plt.plot(x_sensors[xs_include], data_init_subset_sensors[0,xs_include].T, \n",
    "         lw=2, c='b', ls='--',\n",
    "         label='Filtered Predicted Response')\n",
    "plt.scatter(x_sensors[xs_include], noisy_obs_subset_sensors[0,2,xs_include].T,\n",
    "            s=10, marker='x', c='r', alpha=0.25, label='Noisy Observed Data')\n",
    "plt.plot(x_sensors[xs_include], data_obs_subset_sensors[0,2,xs_include].T, \n",
    "         lw=2, c='r', ls='-.',\n",
    "         label='Filtered Observed Response')\n",
    "plt.xlabel('$x$-coordinate')\n",
    "plt.ylabel('Displacement [dimensionless]')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "# plt.title('Responses and Noisy Data', fontsize=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for j in range(num_sets):\n",
    "    learn_list[j].filter_observations(observed_data_coordinates=predicted_data_coordinates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for j in range(num_sets):\n",
    "    obs_maps.append(learn_list[j].classify_and_transform_observations())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pred_maps, obs_maps = [], []\n",
    "for j in range(num_sets):\n",
    "    obs_maps_temp = learn_list[j].transform_observations()\n",
    "    pred_maps.append(learn_list[j].predict_maps)\n",
    "    obs_maps.append(obs_maps_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Generate kernel density estimates on new QoI and calculate new weights\n",
    "pi_predict_kdes = []\n",
    "pi_obs_kdes = []\n",
    "r_vals = []\n",
    "r_means = []\n",
    "for j in range(num_sets):\n",
    "    pi_predict_kdes.append([])\n",
    "    pi_obs_kdes.append([])\n",
    "    r_vals.append([])\n",
    "    r_means.append([])\n",
    "    for i in range(learn_list[j].num_clusters):\n",
    "        pi_predict_kdes[j].append(GKDE(learn_list[j].predict_maps[i].T))\n",
    "        pi_obs_kdes[j].append(GKDE(learn_list[j].obs_maps[i].T))\n",
    "        r_vals[j].append(\n",
    "                    np.divide(\n",
    "                        pi_obs_kdes[j][i](\n",
    "                        learn_list[j].predict_maps[i].T), \n",
    "                        pi_predict_kdes[j][i](\n",
    "                        learn_list[j].predict_maps[i].T)))\n",
    "        r_means[j].append(np.mean(r_vals[j][i]))\n",
    "print(f'Diagnostics: {r_means}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "params = np.vstack((a0_init[:num_init], a1_init[:num_init])).T\n",
    "\n",
    "params_obs = np.vstack((a0_obs, a1_obs)).T\n",
    "\n",
    "param_marginals = []\n",
    "param_marginals_modified = []\n",
    "ic_marginals = []\n",
    "true_param_marginals = []\n",
    "true_ic_marginals = []\n",
    "lam_ptr = []\n",
    "cluster_weights = []\n",
    "modified_r_values = []\n",
    "\n",
    "for j in range(num_sets):\n",
    "    cluster_weights.append([])\n",
    "    lam_ptr.append([])\n",
    "    modified_r_values.append(np.zeros(num_init))\n",
    "    for i in range(learn_list[j].num_clusters):\n",
    "        lam_ptr[j].append(np.where(learn_list[j].predict_labels == i)[0])\n",
    "        cluster_weights[j].append(len(np.where(learn_list[j].obs_labels == i)[0]) / num_obs)\n",
    "        modified_r_values[j][lam_ptr[j][i]] = r_vals[j][i]*cluster_weights[j][i]\n",
    "        \n",
    "for j in range(num_sets):\n",
    "    param_marginals.append([])\n",
    "    param_marginals_modified.append([])\n",
    "    true_param_marginals.append([])\n",
    "    for i in range(params.shape[1]):\n",
    "        true_param_marginals[j].append(GKDE(params_obs[start_idx[j]:start_idx[j]+num_obs,i]))\n",
    "        param_marginals_modified[j].append(GKDE(params[:,i], weights=modified_r_values[j]))\n",
    "        param_marginals[j].append([])\n",
    "        for k in range(learn.num_clusters):\n",
    "            param_marginals[j][i].append(GKDE(params[lam_ptr[j][k], i], weights=r_vals[j][k]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(cluster_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for j in range(num_sets):\n",
    "    weighted_diagnostic = 0\n",
    "    for i in range(learn_list[j].num_clusters):\n",
    "        weighted_diagnostic += cluster_weights[j][i] * r_means[j][i]\n",
    "    print(weighted_diagnostic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "##### param_labels = ['$a_0$', '$a_1$']\n",
    "\n",
    "param_0_y_limits = [-0.25, 8]\n",
    "param_1_y_limits = [-0.25, 6.5]\n",
    "\n",
    "for i in range(params.shape[1]):\n",
    "    if i==0:\n",
    "        x = np.linspace(0.8,1.2,100)\n",
    "    else:\n",
    "        x = np.linspace(-0.2,0.2,100)\n",
    "        \n",
    "    for k in range(num_sets):\n",
    "        fig = plt.figure()\n",
    "        fig.clear()\n",
    "        \n",
    "        plt.plot(x, true_param_marginals[k][i](x), label = 'KDE of DG; set {}'.format(k+1), \n",
    "                 linewidth=2, linestyle='dashed')\n",
    "        \n",
    "        plt.plot(x,1/0.4*np.ones(len(x)), linewidth=1)\n",
    "        \n",
    "        plt.plot(x, param_marginals_modified[k][i](x), \n",
    "                 label = r'Update; set {}'.format(k+1), linewidth=2, linestyle='dashdot')\n",
    "        \n",
    "        if i==1:\n",
    "            plt.axvline(0, color='k', lw=2, ls='-.')\n",
    "        plt.title('Densities for ' + param_labels[i])\n",
    "        plt.ylim(eval('param_{}_y_limits'.format(i)))\n",
    "        plt.legend()\n",
    "        plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.integrate import quad as quad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xmins = [0.8, -0.2]\n",
    "xmaxs = [1.2, 0.2]\n",
    "for i in range(2):\n",
    "    print('~'*100)\n",
    "    print('a'+str(i)+' TV metrics')\n",
    "    print('~'*100)\n",
    "    for j in range(3):\n",
    "        print('Set = ' + str(j) + ', TV metric = {:.3f}'.format(\n",
    "            quad(lambda x: 0.5*np.abs(true_param_marginals[j][i](x) - param_marginals_modified[j][i](x)),\n",
    "                 xmins[i], xmaxs[i],\n",
    "                 full_output=1)[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": true,
   "autocomplete": false,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
