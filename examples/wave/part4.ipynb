{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e006b1b-b180-44f8-9d51-3f82606de0f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from luq.luq import *\n",
    "from scipy.stats import norm, beta # for data-generating distributions\n",
    "from scipy.stats import gaussian_kde as GKDE\n",
    "from scipy.integrate import quadrature # for calculating 1-D TV metrics\n",
    "from tabulate import tabulate # for presenting results for iterative approach\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams.update({'font.size': 12})\n",
    "import matplotlib.tri as tri\n",
    "import ipywidgets as wd\n",
    "\n",
    "# color palette\n",
    "c = ['#377eb8', '#ff7f00', '#4daf4a',\n",
    "                  '#f781bf', '#a65628', '#984ea3',\n",
    "                  '#999999', '#e41a1c', '#dede00']\n",
    "\n",
    "# setup fontsizes for plots\n",
    "plt_params = {'legend.fontsize': 14,\n",
    "          'figure.figsize': (10,8), #(6.4, 4.8),\n",
    "         'axes.labelsize': 16,\n",
    "         'axes.titlesize': 16,\n",
    "         'xtick.labelsize': 14,\n",
    "         'ytick.labelsize': 14}\n",
    "plt.rcParams.update(plt_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b71dee2-8f11-46dd-8b3e-0cf1cb94f642",
   "metadata": {},
   "source": [
    "# Details on Data-Generating Samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a77ff17-a2a7-47ac-8c56-b3566c20d095",
   "metadata": {},
   "source": [
    "## Generating Observed Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6626c478-73f9-484b-903d-92169f29ede4",
   "metadata": {},
   "source": [
    "Data was created in the notebook/script labeled 'generating_data' found with this notebook. The model is the 2-D wave equation $$\\dfrac{\\partial^2 u}{\\partial t^2}=\\dfrac{\\partial^2 u}{\\partial x^2}+\\dfrac{\\partial^2 u}{\\partial y^2}, \\quad \\left(x,y\\right)\\in (0,5)^2$$ with $u=u(x,y,t)$ and boundary conditions $u(0,y,t)=u(x,0,t)=u(5,y,t)=u(x,5,t)=0$. The problem is to model a water droplet at location $(a,b)$ given by $$u(x,y,0)=0.2\\text{exp}\\left(-10\\left(\\left(x-a\\right)^2+\\left(y-b\\right)^2\\right)\\right)$$ where the location $(a,b)$ has some unknown distribution creating uncertain model outputs, and the goal is to quantify the uncertainty in the droplet locations using observed uncertainties in model outputs. The droplet locations are given a data-generating distribution  described by independent distributions for $a$ and $b$ where $a$ is taken to be a Beta(2,5) distribution scaled and shifted to be on the interval $[1,2]$ and $b$ is taken to be a $N(2.5,0.5)$ normal distribution. The data is generated by creating 200 i.i.d. samples from this data-generating distributions and solving the model using a standard centered finite difference scheme on a 101x101 regular uniformly-spaced mesh on $[0,5]^2$ using 0.005 sized time-steps. The data is then extracted at spatial points $(0.5i,0.5j)$ for $i,j=1,\\dots,9$, i.e., on a 9x9 sub-grid of the original 101x101 grid, and at times $t=0.5,1.0,1.5,\\dots,7.0$. We simulate noise on this observed data using a normal distribution centered at 0 with standard deviation 2.5E-3 in both spatial directions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ee06ad-c3ce-463d-acf4-d4f1d0fa4682",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# parameter samples for construction of pi_obs\n",
    "\n",
    "num_obs_samples = 200\n",
    "np.random.seed(12345678)\n",
    "params_obs = np.vstack([2 * np.random.beta(a=2, b=5, size=num_obs_samples) + 1,\n",
    "                         np.random.normal(loc=2.5, scale=0.5, size=num_obs_samples)]) # unknown data-generated parameters corresponding to observed samples\n",
    "\n",
    "obs = np.load('dg_samples/obs', allow_pickle=True) # noisy observed samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5564c4bf-3331-41a5-b041-cec39235274c",
   "metadata": {},
   "source": [
    "## Generating Predicted Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1482a78b-c5f0-4b3d-9161-9e60dda10e38",
   "metadata": {},
   "source": [
    "The predicted data is generated using 1000 i.i.d. uniform samples of $[0,5]^2$ and solving the model as described above, but extracting the data on a finer 49x49 grid at spatial points $(0.1i,0.1j)$ for $i,j=1,\\dots,49$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc148f24-8e2e-4398-a484-16fbda1e7ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameter samples of pi_init\n",
    "\n",
    "num_samples = int(1E3)\n",
    "\n",
    "np.random.seed(123456)\n",
    "params = np.random.uniform(low=0.0,high=5.0,size=(2,num_samples)) # uniformly distributed parameter samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c1b1d8-e5d5-4d29-b18e-5e7980cb01e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# finite-difference scheme\n",
    "\n",
    "# defining model solve function\n",
    "dx = 0.05\n",
    "dy = 0.05\n",
    "dt = 0.005 # satifies CFL condition\n",
    "\n",
    "xn = np.linspace(0,5.0,101) # 101 = length in x / dx\n",
    "ym = np.linspace(0,5.0,101)\n",
    "tk = np.linspace(0,7.0,1401) # 1401 = length in t / dt\n",
    "\n",
    "# defining model solve on 101x101 uniform mesh of [0,5]^2 for t = 0 to t = 7 with dt = 0.005\n",
    "def M(a,b):\n",
    "    # initializing the model solution\n",
    "    # using Dirichlet boundary conditions,so initializing with zeros means boundary values are set\n",
    "    u = np.zeros((101,101,1401))\n",
    "    \n",
    "    # iterate through times; t here is equivalent to time and time index\n",
    "    for t in range(1401):\n",
    "        \n",
    "        # if t = 0, use initial condition modeling wave droplet\n",
    "        if t == 0:\n",
    "            mesh = np.meshgrid(xn[1:-1],ym[1:-1])\n",
    "            u[1:-1,1:-1,t] = 0.2*np.exp(-10*((mesh[0].T-a)**2+(mesh[1].T-b)**2))\n",
    "        \n",
    "        # else solve model using finite-difference scheme\n",
    "        else:\n",
    "            u[1:-1,1:-1,t] = 2 * u[1:-1,1:-1,t-1] - u[1:-1,1:-1,max(0,t-2)] \\\n",
    "                + dt**2 / dx**2 * (u[2:,1:-1,t-1] - 2 * u[1:-1,1:-1,t-1] + u[:-2,1:-1,t-1]) \\\n",
    "                + dt**2 / dy**2 * (u[1:-1,2:,t-1] - 2 * u[1:-1,1:-1,t-1] + u[1:-1,:-2,t-1])\n",
    "    return u\n",
    "\n",
    "# indexing for extracting data on different grid sizes\n",
    "\n",
    "# indexing function for flattening data\n",
    "def idx_at(x,y):\n",
    "    idx = []\n",
    "    idx.append((x / dx).astype(int))\n",
    "    idx.append((y / dy).astype(int))\n",
    "    return idx\n",
    "\n",
    "# using indexing function to extract data on uniformly-spaced mesh given by delta\n",
    "def create_idx(delta):\n",
    "    N = (5-delta)/delta \n",
    "    # note: only delta such that (5-delta)/delta is int can be used (or does not change value when cast as int) \n",
    "    # any other delta value requires extrapolation\n",
    "    pts = np.linspace(delta,5-delta,int(N))\n",
    "    grid_pts = np.meshgrid(pts,pts)\n",
    "    idx = idx_at(grid_pts[0],grid_pts[1])\n",
    "    return [idx[0].flatten(), idx[1].flatten()]\n",
    "\n",
    "# creating grid of size grid_size x grid_size for data coordinates\n",
    "def create_coordinates(grid_size):\n",
    "    delta = 5 / (grid_size + 1)\n",
    "    X, Y = np.meshgrid(range(grid_size),range(grid_size))\n",
    "    X = X / grid_size * (5 - delta) + delta\n",
    "    Y = Y / grid_size * (5 - delta) + delta\n",
    "    return np.vstack([X.flatten(), Y.flatten()]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec782b33-3750-45f2-a68e-21341b29b5f5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# predicted data samples on 49x49 grid, 0.1 mesh size\n",
    "\n",
    "pred = np.zeros((num_samples,49**2,14))\n",
    "idx = create_idx(0.1)\n",
    "for i in range(num_samples):\n",
    "    tmp = M(params[0,i], params[1,i])\n",
    "    pred[i,:,:] = tmp[idx[0],idx[1],100::100]\n",
    "    print(f'Predicted sample {i} done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6947d371-dae5-4bb8-b150-33a939a32625",
   "metadata": {},
   "source": [
    "# Filtering Noisy Spatial Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2806302-32c6-4fa8-b449-b549f40d8ac9",
   "metadata": {},
   "source": [
    "The noisy observed data is filtered by centereing the data with 0 mean in all directions, then fitting a weighted sum of 1-7 Gaussians iteratively, starting with 1 Gaussian and increasing the number of Gaussian's used until either 7 Gaussians are used or a relative error is within a tolerance of $10^{-4}$. This is all done within the $\\texttt{LUQ}$ package that uses the new supplemental $\\texttt{RBFFit}$ package within $\\texttt{LUQ}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef8976b7-c56f-4daa-af55-f3799f7d5d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # code used to filter data on 9x9 grid of spatial locations\n",
    "\n",
    "# # instantiate LUQ at each time step\n",
    "# learn = [[None]]*4 # excluding first 4 time steps\n",
    "\n",
    "# for i in range(4,obs.shape[-1]): # observed data has shape (num_samples,num_spatial_locations,num_time_steps)\n",
    "#     pred_data = pred[:,:,i]\n",
    "#     obs_data = obs[:,:,i]\n",
    "#     learn.append(LUQ(predicted_data=pred_data,\n",
    "#                       observed_data=obs_data))\n",
    "\n",
    "# # filtering observed data at each time step using sum of Gaussians\n",
    "\n",
    "# data_coordinates = create_coordinates(9)\n",
    "\n",
    "# np.random.seed(44444)\n",
    "# import pickle\n",
    "\n",
    "# # iterate over time steps\n",
    "# for i in range(2,obs.shape[-1]):\n",
    "#     learn[i].filter_data(filter_method='rbfs',\n",
    "#                           filtered_data_coordinates=data_coordinates,\n",
    "#                           num_rbf_list=range(1,8),\n",
    "#                           initializer='kmeans',\n",
    "#                           max_opt_count=10,\n",
    "#                           filter_predictions=False,\n",
    "#                           verbose=True)\n",
    "#     fn = 'post_filtering/params_t' + str(i)\n",
    "#     with open(fn, 'wb') as pf:\n",
    "#         pickle.dump(learn[i].filtered_obs_params, pf)\n",
    "\n",
    "# loading pre-computed filtered data as luq instance\n",
    "import pickle\n",
    "\n",
    "learn = [[None]]*4 # excluding first 4 time steps\n",
    "for i in range(4,obs.shape[-1]):\n",
    "    fn = 'post_filtering/params_t' + str(i)\n",
    "    with open(fn, 'rb') as pf:\n",
    "        filtered_obs_params = pickle.load(pf)\n",
    "    # re-instantiating LUQ\n",
    "    pred_data = pred[:,:,i]\n",
    "    obs_data = obs[:,:,i]\n",
    "    learn.append(LUQ(pred_data,\n",
    "                     obs_data))\n",
    "    learn[i].filtered_obs = obs_data # needed for shape of filtered_obs when re-evaluating using new_data_coordinates\n",
    "    learn[i].filtered_obs_params = filtered_obs_params # fitted parameters\n",
    "\n",
    "    # updating obs_filtering_params info; only need filter_method, remove_trend, add_poly, and poly_deg for recreating data\n",
    "    learn[i].info['obs_filtering_params'] = {'filter_method': 'rbfs',\n",
    "                                          'num_rbf_list': range(1,8),\n",
    "                                          'remove_trend': False,\n",
    "                                          'add_poly': False,\n",
    "                                          'poly_deg': None,\n",
    "                                          'initializer': 'kmeans',\n",
    "                                          'max_opt_count': 10,\n",
    "                                          'tol': 0.0001}\n",
    "\n",
    "    # creating data coordinates where observed data was created\n",
    "    learn[i].observed_data_coordinates = create_coordinates(9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d03c4a20-a4d9-49e5-9156-c40031337284",
   "metadata": {},
   "source": [
    "## Matching Dimensionality of Predicted and Observed Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f8690d-792d-4ef8-b7af-5d7abbf0d670",
   "metadata": {},
   "source": [
    "We need the observed and predicted data to exist in the same dimensional space. At this point, the observed data is in a $9^2$ dimensional space while the predicted data is in a $49^2$ dimensional space. Using the fitted function parameters learned with the above filtering step, we re-evaluate the observed data on the finer 49x49 grid to match dimensionality of the observed and predicted data spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c67ba5-fc2e-432b-9e47-b47506833034",
   "metadata": {},
   "outputs": [],
   "source": [
    "# re-evaluating filtered observations on same grid as predictions\n",
    "\n",
    "data_coordinates = create_coordinates(49)\n",
    "\n",
    "for t in range(4,obs.shape[-1]):\n",
    "    learn[t].filtered_predictions = pred[:,:,t]\n",
    "    learn[t].new_data_coordinates(data_coordinates, \n",
    "                                  recalc_pred=False)\n",
    "    \n",
    "    print(f'Predicted data shape: {learn[t].predicted_data.shape}')\n",
    "    print(f'Filtered observed data shape: {learn[t].filtered_obs.shape}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b1f3857-0684-45bb-bc79-96b0a7fd13db",
   "metadata": {},
   "source": [
    "# Learning QoI Map and Transforming Data to Learned QoI Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bdf7064-98f4-4419-b3ba-4b4f3c7a307a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# learning 2 QoI's from data using kernel pca and transforming the data into QoI samples\n",
    "\n",
    "for t in range(4,obs.shape[-1]):\n",
    "    learn[t].learn_qois_and_transform(num_qoi=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69498933-f5cf-4b6e-aba9-40b4386089a7",
   "metadata": {},
   "source": [
    "# Computing DCI Solution Iteratively"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e275145-21ad-459d-bc62-d3f3dd4306b4",
   "metadata": {},
   "source": [
    "## Iterative Algorithm Details"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1e28eb7-60c7-4b86-91c9-5a7c3011a152",
   "metadata": {},
   "source": [
    "The iterative approach iterates through each time step, calculates the corresponding samples of $$r\\left(a,b\\right):=\\dfrac{\\pi_{\\text{obs}}\\left(Q\\left(a,b\\right)\\right)}{\\pi_{\\text{pred}}\\left(Q\\left(a,b\\right)\\right)}$$ where $Q$ represents the QoI map on the parameters $a$ and $b$. This is done first using both QoI components, but if the DCI diagnostic is not within the specified error tolerance, then the procedure is restarted using one less QoI component until no components are left or until error tolerance is reached. There is then an optional 'quality' check that can be performed to decide whether the iteration should be included or not. In the code below, the algorithm without the quality check is labeled as alg=0 while the algorithm with the quality check is labeled as alg=1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "800c622b-0eb2-47a0-935f-9039e4baba52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate kernel density estimates on new QoI and calculate new weights\n",
    "\n",
    "num_qoi = 2\n",
    "tol = 0.095\n",
    "r_vals = []\n",
    "r_means = []\n",
    "\n",
    "times_used = []\n",
    "num_qoi_used = []\n",
    "\n",
    "for alg in [0,1]: # computing solution without and with optional quality check\n",
    "    r_vals.append([])\n",
    "    r_means.append([])\n",
    "    times_used.append([])\n",
    "    num_qoi_used.append([])\n",
    "    for t in range(4,obs.shape[-1]): # excluding first time steps due to too low signal-to-noise ratio\n",
    "        for n in reversed(range(1,num_qoi+1)): # iterate over number of QoI from max 2 down to 0\n",
    "            if len(r_vals[alg]) == 0:\n",
    "                weights = np.ones(learn[t].predict_maps[0][:,:n].shape[0]) # weights of all ones for initial iteration\n",
    "            else:\n",
    "                weights = np.prod(r_vals[alg], axis=0) # weights from previous time step for non-initial iterations\n",
    "            pi_pred = GKDE(learn[t].predict_maps[0][:,:n].T, weights=weights) # computing new predicted density on learned QoI\n",
    "            pi_obs = GKDE(learn[t].obs_maps[0][:,:n].T) # computing observed density\n",
    "            # computing new r-values associated with learned QoI\n",
    "            r_vals[alg].append(\n",
    "                np.divide(\n",
    "                    pi_obs(\n",
    "                        learn[t].predict_maps[0][:,:n].T), \n",
    "                    pi_pred(\n",
    "                        learn[t].predict_maps[0][:,:n].T))) \n",
    "            r = weights*r_vals[alg][-1] # new proposed r-samples\n",
    "            r_means[alg].append(np.mean(r))\n",
    "            if np.abs(1-r_means[alg][-1]) <= tol: # checking DCI diagnostic\n",
    "                if alg == 0 or np.mean(r**2) > np.mean(weights**2): # alg=0 will use data; alg=1 will only use data if optional quality check holds\n",
    "                    times_used[alg].append(0.5*(t+1))               \n",
    "                    num_qoi_used[alg].append(n)\n",
    "                else:\n",
    "                    r_vals[alg].pop()\n",
    "                    r_means[alg].pop()        \n",
    "                break # breaks if DCI diagnostic is within tolerance; otherwise, r is re-computed using fewer QoI components with previous results removed\n",
    "            else:\n",
    "                r_vals[alg].pop() \n",
    "                r_means[alg].pop()\n",
    "\n",
    "# presenting results for both algorithms\n",
    "tables = []\n",
    "for alg in [0,1]:\n",
    "    tables.append([])\n",
    "    for i in range(len(r_vals[alg])):\n",
    "        r = np.prod(r_vals[alg][:i+1],axis=0) # r samples at each iteration\n",
    "        tables[alg].append([i+1,times_used[alg][i],num_qoi_used[alg][i],r_means[alg][i],np.mean(r**2)])\n",
    "    print(f'Results for algorithm {alg}:')\n",
    "    print(tabulate(tables[alg], headers=['iteration','time','number of QoIs used','E_init(r)','E_update(r)']))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e4c71d0-e809-4ce9-93ee-3fc6ecbe3a90",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4baa54a-6acb-4db3-9de1-ef2be245fa30",
   "metadata": {},
   "source": [
    "## Visualizing Updated Densities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02cef8d9-7f94-4abf-b1c5-f913901a1623",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# defining uniform distribution for initial density \n",
    "def unif_dist(x, p_range):\n",
    "    y = np.zeros(x.shape)\n",
    "    val = 1.0/(p_range[1] - p_range[0])\n",
    "    for i, xi in enumerate(x):\n",
    "        if xi < p_range[0] or xi >  p_range[1]:\n",
    "            y[i] = 0\n",
    "        else:\n",
    "            y[i] = val\n",
    "    return y\n",
    "\n",
    "# calculating eact data-generating marginals\n",
    "exact_param_marginals = [lambda x : beta.pdf((x-1)/2,2,5)/2,\n",
    "                         lambda x : norm.pdf(x,2.5,0.5)]\n",
    "\n",
    "# calculating exact data-generating joint\n",
    "np.random.seed(1234) # for reproducibility\n",
    "params_graphing = np.random.uniform(low=0.0,high=5.0,size=(2,10000)) # large number of uniform parameter samples for graphing\n",
    "\n",
    "exact_dg = lambda x, y : exact_param_marginals[0](x)*exact_param_marginals[1](y)\n",
    "exact_dg = exact_dg(params_graphing[0,:],params_graphing[1,:])\n",
    "kde_dg = GKDE(params_obs)(params_graphing)\n",
    "\n",
    "# KDEs of true marginals\n",
    "kde_param_marginals = []\n",
    "for i in range(params.shape[0]):\n",
    "        kde_param_marginals.append(GKDE(params_obs[i,:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ebfe4a3-24fe-455f-9914-e74b0bb8e9dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# constructing and plotting updated marginals\n",
    "\n",
    "x_min = 0.0\n",
    "x_max = 5.0\n",
    "delta = 0.25*(x_max - x_min)\n",
    "x = np.linspace(x_min-delta, x_max+delta, 100)\n",
    "param_labels = [r'$a$', r'$b$']\n",
    "param_marginals = []\n",
    "param_str = ['a', 'b']\n",
    "\n",
    "for alg in [0,1]:\n",
    "    param_marginals.append([])\n",
    "    for i in range(params.shape[0]):\n",
    "        plt.figure()\n",
    "        plt.plot(x, unif_dist(x,[0.0,5.0]), label='Initial', linewidth=2, c=c[0])\n",
    "        param_marginals[alg].append(GKDE(params[i,:], weights=np.prod(r_vals[alg], axis=0)))\n",
    "        mar = param_marginals[alg][i](x)\n",
    "        plt.plot(x, mar, label = 'Updated', linewidth=4, linestyle='dashed', c=c[1])\n",
    "        plt.plot(x, exact_param_marginals[i](x), label='Data-generating', linewidth=4, linestyle='dotted', c=c[2])\n",
    "        plt.title('Densities for parameter '+param_labels[i]+f' using algorithm {alg+1}')\n",
    "        plt.xlabel(param_labels[i])\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "#         if alg == 0:\n",
    "#             fn = 'plots/wave_marginal_' + param_str[i] + '_4.png'\n",
    "#         else:\n",
    "#             fn = 'plots/wave_marginal_' + param_str[i] + '_4_r2.png'\n",
    "#         plt.savefig(fn, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a6d1504-4908-40e8-8a1e-9fa9715dc086",
   "metadata": {},
   "source": [
    "## Visualizing Iterative Updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95781edf-5085-41d6-a1db-d8f1e8389978",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting updated marginals at each iteration\n",
    "\n",
    "param_marginals = []\n",
    "\n",
    "for alg in [0,1]:\n",
    "    param_marginals.append([[],[]])\n",
    "    for i in range(params.shape[0]):\n",
    "        plt.figure()\n",
    "        plt.plot(x, unif_dist(x,[0.0,5.0]), label='Initial', linewidth=2, c=c[2])\n",
    "        plt.plot(x, exact_param_marginals[i](x), label='Data-generating', linewidth=2, linestyle='dotted', c=c[1])\n",
    "        # plt.plot(x, kde_param_marginals[i](x), label='KDE', linewidth=4, c=c[1])\n",
    "        for j in range(len(r_vals[alg])):\n",
    "            param_marginals[alg][i].append(GKDE(params[i,:], weights=np.prod(r_vals[alg][:j+1],axis=0)))\n",
    "            if j == len(r_vals)-1:\n",
    "                plt.plot(x, param_marginals[alg][i][j](x), label=f'final iteration', linewidth=2, c=c[0], alpha=(j+1)/len(r_vals[alg]))\n",
    "            else:\n",
    "                plt.plot(x, param_marginals[alg][i][j](x), label=f'iteration {j+1}', linewidth=2, linestyle='dashed', c=c[0], alpha=(j+1)/len(r_vals[alg]))\n",
    "        plt.title('Updated densities for parameter '+param_labels[i]+f' using algorithm {alg+1}')\n",
    "        plt.xlabel(param_labels[i])\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "#         if alg == 0:\n",
    "#             fn = 'plots/wave_marginal_' + param_str[i] + '_iter.png'\n",
    "#         else:\n",
    "#             fn = 'plots/wave_marginal_' + param_str[i] + '_iter_r2.png'\n",
    "#         plt.savefig(fn, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3bc664-0d8c-441f-befa-5efea45dccec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# color plot of updated density\n",
    "\n",
    "pi_updates = []\n",
    "for alg in [0,1]:\n",
    "    pi_updates.append(GKDE(params, weights=np.prod(r_vals[alg], axis=0))(params_graphing))\n",
    "    plt.figure()\n",
    "    plt.scatter(params_graphing[0,:], params_graphing[1,:], c=pi_updates[alg])\n",
    "    plt.scatter(params_obs[0,:], params_obs[1,:], c='xkcd:black', s=10, label='data-generating samples')\n",
    "    plt.legend()\n",
    "    plt.xlabel(param_labels[0])\n",
    "    plt.ylabel(param_labels[1])\n",
    "    plt.title(f'Color plot of updated density using algorithm {alg+1}')\n",
    "    plt.colorbar(label='density')\n",
    "    plt.tight_layout()\n",
    "    # if alg == 0:\n",
    "    #     fn = 'plots/wave_joint_4.png'\n",
    "    # else:\n",
    "    #     fn = 'plots/wave_joint_4_r2.png'\n",
    "    # plt.savefig(fn, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ec81a4-f47c-4f12-a0c8-018e4795a590",
   "metadata": {},
   "source": [
    "## Quantifying Differences Between DCI solution and True DG Densities Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "866ed1df-6845-4633-b47a-76ae9decab3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating TV metric between updated and exact joint distributions\n",
    "\n",
    "for alg in [0,1]:\n",
    "    TV_final = np.abs(pi_updates[alg]-exact_dg)/2\n",
    "    # TV = np.abs(pi_updates[alg]-kde_dg)/2\n",
    "    TV_final = np.mean(TV_final)*25\n",
    "    print(f'TV metric between pi_update and data-generating joint distribution for algorithm {alg+1}: {TV_final}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac0a548-a4de-459f-82e3-3f8f4ee54fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating TVs for marginals at each iteration\n",
    "\n",
    "marginal_TVs = []\n",
    "TVs = []\n",
    "for alg in [0,1]:\n",
    "    marginal_TVs.append([[],[]])\n",
    "    for i in range(params.shape[0]):\n",
    "        # diff = lambda x : np.abs(unif_dist(x,[0.0,5.0])-exact_param_marginals[i](x))\n",
    "        diff = lambda x : np.abs(unif_dist(x,[0.0,5.0])-kde_param_marginals[i](x))\n",
    "        TV, _ = quadrature(diff, 0.0, 5.0, tol=1e-2)\n",
    "        marginal_TVs[alg][i].append(TV/2)\n",
    "    for i in range(params.shape[0]):\n",
    "        for j in range(len(r_vals[alg])):\n",
    "            # diff = lambda x : np.abs(param_marginals[alg][i][j](x)-exact_param_marginals[i](x))\n",
    "            diff = lambda x : np.abs(param_marginals[alg][i][j](x)-kde_param_marginals[i](x))\n",
    "            TV, _ = quadrature(diff, 0.0, 5.0, tol=1e-2)\n",
    "            marginal_TVs[alg][i].append(TV/2)\n",
    "\n",
    "    TVs.append([])\n",
    "    pi_init = lambda x, y : unif_dist(x,[0.0,5.0]) * unif_dist(y,[0.0,5.0])\n",
    "    pi_init = pi_init(params_graphing[0,:], params_graphing[1,:])\n",
    "    # TV = np.abs(pi_init-exact_dg)/2\n",
    "    TV = np.abs(pi_init-kde_dg)/2\n",
    "    TV = np.mean(TV)*25\n",
    "    TVs[alg].append(TV)\n",
    "    for j in range(len(r_vals[alg])):\n",
    "        pi_update = GKDE(params, weights=np.prod(r_vals[alg][:j+1], axis=0))(params_graphing)\n",
    "        # TV = np.abs(pi_update-exact_dg)/2\n",
    "        TV = np.abs(pi_update-kde_dg)/2\n",
    "        TV = np.mean(TV)*25\n",
    "        TVs[alg].append(TV)\n",
    "\n",
    "min_marginal_TVs = []\n",
    "for i in range(params.shape[0]):\n",
    "    diff = lambda x : np.abs(exact_param_marginals[i](x)-kde_param_marginals[i](x))\n",
    "    TV, _ = quadrature(diff, 0.0, 5.0, tol=1e-2)\n",
    "    min_marginal_TVs.append(TV/2)\n",
    "\n",
    "min_TV = np.abs(kde_dg-exact_dg)/2\n",
    "min_TV = np.mean(min_TV)*25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eec7a16-7d9b-46c3-9974-2671d4676888",
   "metadata": {},
   "outputs": [],
   "source": [
    "# printing TVs at final iteration\n",
    "\n",
    "for alg in [0,1]:\n",
    "    for i in range(params.shape[0]):\n",
    "        print(f'TV metric for algorithm {alg+1} between final iteration and DG marginals for {param_labels[i]}: {str(marginal_TVs[alg][i][-1])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb9ca06-8179-4b68-ba27-e2dcb7350a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting TV per iteration\n",
    "\n",
    "for alg in [0,1]:\n",
    "    n = range(len(TVs[alg]))\n",
    "    plt.figure()\n",
    "    plt.scatter(n,TVs[alg],c=c[0])\n",
    "    plt.plot(n,TVs[alg],c=c[0],label='TV between updated and exact densities')\n",
    "    plt.hlines(min_TV,0,len(TVs[alg])-1,linestyles='dashed',colors=c[0],label='TV between exact and KDE of data-generating densities')\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('TV')\n",
    "    plt.ylim([0,1])\n",
    "    plt.title(f'TV metric between updated and exact joint parameter densities for algorithm {alg+1}')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "\n",
    "    for i, t in enumerate(times_used[alg]):\n",
    "        txt = 't=' + str(t)\n",
    "        plt.annotate(txt, (n[i+1]+0.1, TVs[alg][i+1]+0.02), size=12)\n",
    "        \n",
    "#     if alg == 0:\n",
    "#         fn = 'plots/wave_TV_joint.png'\n",
    "#     else:\n",
    "#         fn = 'plots/wave_TV_joint_r2.png'\n",
    "#     plt.savefig(fn, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b1cf2e-0029-4fff-9424-81eb8bbe2f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting marginal TVs\n",
    "        \n",
    "for alg in [0,1]:\n",
    "    n = range(len(marginal_TVs[alg][0]))\n",
    "    plt.figure()\n",
    "    for i in range(params.shape[0]):\n",
    "        plt.scatter(n,marginal_TVs[alg][i], c=c[i], label='TV between updated and exact marginals for '+param_labels[i])\n",
    "        plt.plot(n,marginal_TVs[alg][i], c=c[i])\n",
    "        plt.hlines(min_marginal_TVs[i], 0, len(marginal_TVs[alg][0])-1, linestyles='dashed', colors=c[i],label='TV between exact and KDE of data-generating marginals for '+param_labels[i])\n",
    "    plt.legend()\n",
    "    plt.xlabel('iteration')\n",
    "    plt.ylabel('TV')\n",
    "    plt.ylim([0,1])\n",
    "    plt.title(f'TV metric between updated and exact marignal parameter densities for algorithm {alg+1}')\n",
    "    plt.tight_layout()\n",
    "\n",
    "    for i, t in enumerate(times_used[alg]):\n",
    "        txt = 't=' + str(t)\n",
    "        plt.annotate(txt, (n[i+1]+0.1, np.max(marginal_TVs[alg],axis=0)[i+1]+0.02), size=12)\n",
    "        \n",
    "#     if alg == 0:\n",
    "#         fn = 'plots/wave_TV_marginals.png'\n",
    "#     else:\n",
    "#         fn = 'plots/wave_TV_marginals_r2.png'\n",
    "#     plt.savefig(fn, bbox_inches='tight')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
