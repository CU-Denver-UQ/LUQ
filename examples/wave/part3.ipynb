{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41668331-76a6-4b10-8ce8-972d18cac59c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from luq.luq import *\n",
    "from scipy.stats import norm, beta\n",
    "from scipy.stats import gaussian_kde as GKDE\n",
    "from scipy.integrate import quadrature\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.tri as tri\n",
    "import ipywidgets as wd\n",
    "\n",
    "# colorblind friendly color palette\n",
    "c = ['#377eb8', '#ff7f00', '#4daf4a',\n",
    "                  '#f781bf', '#a65628', '#984ea3',\n",
    "                  '#999999', '#e41a1c', '#dede00']\n",
    "\n",
    "# setup fontsizes for plots\n",
    "plt_params = {'legend.fontsize': 14,\n",
    "          'figure.figsize': (10,8), #(6.4, 4.8),\n",
    "         'axes.labelsize': 16,\n",
    "         'axes.titlesize': 16,\n",
    "         'xtick.labelsize': 14,\n",
    "         'ytick.labelsize': 14}\n",
    "plt.rcParams.update(plt_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "393a7fac-a09b-4485-9962-7b95bcdd93f2",
   "metadata": {},
   "source": [
    "# Details on Data-Generating Samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d75b9b6f-2960-48f8-b4ec-501b9496670f",
   "metadata": {},
   "source": [
    "## Generating Observed Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f386910-36fa-4045-849c-6be55b54a2ee",
   "metadata": {},
   "source": [
    "Data was created in the notebook/script labeled 'generating_data' found with this notebook. The model is the 2-D wave equation $$\\dfrac{\\partial^2 u}{\\partial t^2}=\\dfrac{\\partial^2 u}{\\partial x^2}+\\dfrac{\\partial^2 u}{\\partial y^2}, \\quad \\left(x,y\\right)\\in (0,5)^2$$ with $u=u(x,y,t)$ and boundary conditions $u(0,y,t)=u(x,0,t)=u(5,y,t)=u(x,5,t)=0$. The problem is to model a water droplet at location $(a,b)$ given by $$u(x,y,0)=0.2\\text{exp}\\left(-10\\left(\\left(x-a\\right)^2+\\left(y-b\\right)^2\\right)\\right)$$ where the location $(a,b)$ has some unknown distribution creating uncertain model outputs, and the goal is to quantify the uncertainty in the droplet locations using observed uncertainties in model outputs. The droplet locations are given a data-generating distribution  described by independent distributions for $a$ and $b$ where $a$ is taken to be a Beta(2,5) distribution scaled and shifted to be on the interval $[1,2]$ and $b$ is taken to be a $N(2.5,0.5)$ normal distribution. The data is generated by creating 200 i.i.d. samples from this data-generating distributions and solving the model using a standard centered finite difference scheme on a 101x101 regular uniformly-spaced mesh on $[0,5]^2$ using 0.005 sized time-steps. The data is then extracted at spatial points $(0.5i,0.5j)$ for $i,j=1,\\dots,9$, i.e., on a 9x9 sub-grid of the original 101x101 grid at time $t=2.5$. We simulate noise on this observed data using a normal distribution centered at 0 with standard deviation 2.5E-3 in both spatial directions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd1b959-9eb1-4c14-ab38-a5d762107faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameter samples for construction of pi_obs\n",
    "\n",
    "num_obs_samples = 200\n",
    "\n",
    "params_obs = np.load('data/params_obs', allow_pickle=True)\n",
    "obs = np.load('data/obs', allow_pickle=True)\n",
    "\n",
    "obs_data = obs[:,:,4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61864a2b-10a1-4589-b406-142a8d62d909",
   "metadata": {},
   "source": [
    "## Generating Predicted Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa9afbf9-664e-443a-b8c0-ac60887d009a",
   "metadata": {},
   "source": [
    "The predicted data is generated using 1000 i.i.d. uniform samples of $[0,5]^2$ and solving the model as described above, then extracting the data at the spatial locations $(0.1i,0.1j)$ for $i,j=1,\\dots,49$ at $t=2.5$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb26f29d-2677-4f02-a743-2e8a49dfb65f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameter samples of pi_init\n",
    "\n",
    "num_samples = int(1E3)\n",
    "\n",
    "params = np.load('data/params', allow_pickle=True)\n",
    "pred = np.load('data/pred_49x49', allow_pickle=True)\n",
    "\n",
    "pred_data = pred[:,:,4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d2987d4-fcec-43c6-b3b3-c78b1a882231",
   "metadata": {},
   "source": [
    "# Filtering Noisy Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef65726a-7741-441a-88cd-4908a70b7876",
   "metadata": {},
   "source": [
    "The noisy observed data is filtered by centereing the data with 0 mean in all directions, then fitting a weighted sum of 1-7 Gaussians iteratively, starting with 1 Gaussian and increasing the number of Gaussian's used until either 7 Gaussians are used or a relative error is within a tolerance of $10^{-4}$. This is all done within the $\\texttt{LUQ}$ package that uses the new supplemental $\\texttt{RBFFit}$ package within $\\texttt{LUQ}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5007f4de-cc1d-4712-be2e-3987381404c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # instantiating luq\n",
    "\n",
    "# learn = LUQ(pred_data,\n",
    "#              obs_data)\n",
    "\n",
    "# # filtering observed data using sum of Gaussians\n",
    "\n",
    "# # defining data coordiates where observed data is taken from\n",
    "# grid_size = 9\n",
    "# delta = 5 / (grid_size + 1)\n",
    "# X, Y = np.meshgrid(range(grid_size),range(grid_size))\n",
    "# X = X / grid_size * (5 - delta) + delta\n",
    "# Y = Y / grid_size * (5 - delta) + delta\n",
    "# data_coordinates = np.vstack([X.flatten(), Y.flatten()]).T\n",
    "\n",
    "# set seed for reproducibility\n",
    "# np.random.seed(333)\n",
    "\n",
    "# # filter data; code takes a long time (maybe up to an hour), so luq instance is saved after to be loaded for future runs\n",
    "# learn.filter_data(filter_method='rbfs',\n",
    "#                    filtered_data_coordinates=data_coordinates,\n",
    "#                    num_rbf_list=range(1,8),\n",
    "#                    initializer='kmeans',\n",
    "#                    max_opt_count=10,\n",
    "#                    filter_predictions=False,\n",
    "#                    verbose=True)\n",
    "\n",
    "# learn.save_instance('instances/part3')\n",
    "\n",
    "# loading pre-computed data\n",
    "\n",
    "import pickle\n",
    "\n",
    "pf = open('instances/part3','rb')\n",
    "learn = pickle.load(pf)\n",
    "pf.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d6beb3-ba07-48b7-a27f-4227d3dff0f9",
   "metadata": {},
   "source": [
    "## Matching Dimensionality of Predicted and Observed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a2ec07-4586-4315-83b7-aa0b143b3026",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# re-evaluating observed data on finer mesh\n",
    "\n",
    "grid_size = 49\n",
    "delta = 5 / (grid_size + 1)\n",
    "X, Y = np.meshgrid(range(grid_size),range(grid_size))\n",
    "X = X / grid_size * (5 - delta) + delta\n",
    "Y = Y / grid_size * (5 - delta) + delta\n",
    "data_coordinates = np.vstack([X.flatten(), Y.flatten()]).T\n",
    "\n",
    "learn.filtered_predictions = pred[:,:,4] # updating predictions to use those on 49x49 grid\n",
    "\n",
    "learn.new_data_coordinates(data_coordinates, \n",
    "                           recalc_pred=False)\n",
    "\n",
    "print(f'Predicted data shape: {learn.filtered_predictions.shape}')\n",
    "print(f'Filtered observed data shape: {learn.filtered_obs.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a3de713-c99d-46df-8132-2390cd4c9554",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualizing filtered surfaces\n",
    "\n",
    "# sample = np.random.randint(0, obs.shape[0])\n",
    "sample = 20 # sample used in paper\n",
    "num_rbfs = learn.filtered_obs_params[sample]['weights'].shape[0]\n",
    "\n",
    "grid_size_orig = 9\n",
    "delta_orig = 5 / (grid_size_orig + 1)\n",
    "X_orig, Y_orig = np.meshgrid(range(grid_size_orig),range(grid_size_orig))\n",
    "X_orig = X_orig / grid_size_orig * (5 - delta_orig) + delta_orig\n",
    "Y_orig = Y_orig / grid_size_orig * (5 - delta_orig) + delta_orig\n",
    "data_coordinates_orig = np.vstack([X_orig.flatten(), Y_orig.flatten()]).T\n",
    "\n",
    "fitted_mesh = np.zeros((grid_size,grid_size))\n",
    "i = -1\n",
    "for k in range(grid_size**2):\n",
    "    j = k % grid_size\n",
    "    if j == 0:\n",
    "        i += 1\n",
    "    fitted_mesh[i,j] = learn.filtered_obs[sample,k]\n",
    "    \n",
    "orig = np.zeros((grid_size_orig,grid_size_orig))\n",
    "i = -1\n",
    "for k in range(grid_size_orig**2):\n",
    "    j = k % grid_size_orig\n",
    "    if j == 0:\n",
    "        i += 1\n",
    "    orig[i,j] = obs[sample,k,4]\n",
    "\n",
    "from matplotlib import cm\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = Axes3D(fig, computed_zorder=False, auto_add_to_figure=False)\n",
    "ax = fig.add_axes(ax)\n",
    "ax.scatter(X_orig, Y_orig, orig, c=c[1], s=50.0, label='true data points')\n",
    "ax.plot_surface(X, \n",
    "                Y, \n",
    "                fitted_mesh, \n",
    "                rstride=1, \n",
    "                cstride=1,\n",
    "                cmap=cm.Blues, \n",
    "                edgecolor='none',\n",
    "                alpha=0.6)\n",
    "ax.set_xlabel(r'$x$')\n",
    "ax.set_ylabel(r'$y$')\n",
    "ax.set_zlabel('wave height')\n",
    "# fig.savefig('plots/filter_step.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efdd587d-7793-44a4-a8ae-bf6a0ee01489",
   "metadata": {},
   "source": [
    "# Learning QoI Map and Transforming Data to Learned QoI Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b250653-4422-48d3-9e1e-4840b4fb7f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning 2 QoI's from data using kernel pca and transforming the data into QoI samples\n",
    "\n",
    "pred_maps, obs_maps = learn.learn_qois_and_transform(num_qoi=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f203fc-5fb9-43a0-9c0f-589b88855fff",
   "metadata": {},
   "source": [
    "# Computing DCI Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b1e78f-47c8-4c63-b169-e7808aee3be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate kernel density estimates on new QoI and calculate new weights\n",
    "\n",
    "pi_predict_kdes = []\n",
    "pi_obs_kdes = []\n",
    "r_vals = []\n",
    "r_means = []\n",
    "for i in range(learn.num_clusters):\n",
    "    pi_predict_kdes.append(GKDE(learn.predict_maps[i].T))\n",
    "    pi_obs_kdes.append(GKDE(learn.obs_maps[i].T))\n",
    "    r_vals.append(\n",
    "        np.divide(\n",
    "            pi_obs_kdes[i](\n",
    "                learn.predict_maps[i].T), \n",
    "            pi_predict_kdes[i](\n",
    "                learn.predict_maps[i].T)))\n",
    "    r_means.append(np.mean(r_vals[i]))\n",
    "    \n",
    "print(f'Diagnostics: {r_means}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "468f894d-66a5-49fe-9e85-8796b40a356f",
   "metadata": {},
   "source": [
    "## Investigating spectral gap of SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29467388-f109-45d6-8814-dbd72b33eaa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # plotting spectral gap for kernel PCA\n",
    "\n",
    "# %reset -f out\n",
    "\n",
    "# def plot_gap(all_eig_vals, n, cluster):\n",
    "#     fig = plt.figure()\n",
    "#     fig.clear()\n",
    "#     #Plotting until maximum number of knots\n",
    "#     eig_vals = all_eig_vals[cluster].eigenvalues_[0:10]\n",
    "#     plt.semilogy(np.arange(np.size(eig_vals))+1,eig_vals/np.sum(eig_vals)*100, marker='.', markersize=20, linestyle='')\n",
    "#     plt.semilogy(np.arange(np.size(eig_vals))+1,eig_vals[n]/np.sum(eig_vals)*100*np.ones(np.size(eig_vals)), 'k--')\n",
    "#     plt.semilogy(np.arange(np.size(eig_vals))+1,eig_vals[n+1]/np.sum(eig_vals)*100*np.ones(np.size(eig_vals)), 'r--')\n",
    "#     plt.text(n+1, eig_vals[n]/np.sum(eig_vals)*150, \n",
    "#              r'%2.3f' %(np.sum(eig_vals[0:n+1])/np.sum(eig_vals)*100) + '% of variation explained by first ' + '%1d' %(n+1) + ' PCs.', \n",
    "#                                                                {'color': 'k', 'fontsize': 14})\n",
    "#     plt.text(n+2, eig_vals[n+1]/np.sum(eig_vals)*150, \n",
    "#              r'Order of magnitude of gap is %4.2f.' %(np.log10(eig_vals[n])-np.log10(eig_vals[n+1])), \n",
    "#                                                                {'color': 'r', 'fontsize': 14})\n",
    "#     s = 'Determining QoI for cluster #%1d' %(cluster+1)\n",
    "#     plt.title(s)\n",
    "#     plt.xlabel('Principal Component #')\n",
    "#     plt.ylabel('% of Variation')\n",
    "#     plt.xlim([0.1, np.size(eig_vals)+1])\n",
    "#     plt.ylim([1e-5,500])\n",
    "\n",
    "\n",
    "# wd.interact(plot_gap, all_eig_vals=wd.fixed(learn.kpcas),\n",
    "#             n = wd.IntSlider(value=0, min=0, max=5),\n",
    "#             cluster = wd.IntSlider(value=0, min=0, max=learn.num_clusters-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a813ea24-a082-4f95-83b6-3e13faf09079",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce10dd0-d8aa-4d9f-a9bb-78b752084ccf",
   "metadata": {},
   "source": [
    "## Visualizing Solution Compared to Initial Densities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d74091c1-7b00-4f51-a650-a508ee1887d4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# defining uniform distribution for initial density \n",
    "def unif_dist(x, p_range):\n",
    "    y = np.zeros(x.shape)\n",
    "    val = 1.0/(p_range[1] - p_range[0])\n",
    "    for i, xi in enumerate(x):\n",
    "        if xi < p_range[0] or xi >  p_range[1]:\n",
    "            y[i] = 0\n",
    "        else:\n",
    "            y[i] = val\n",
    "    return y\n",
    "\n",
    "# calculating eact data-generating marginals\n",
    "exact_param_marginals = [lambda x : beta.pdf((x-1)/2,2,5)/2,\n",
    "                         lambda x : norm.pdf(x,2.5,0.5)]\n",
    "\n",
    "# calculating exact data-generating joint\n",
    "np.random.seed(1234) # for reproducibility\n",
    "params_graphing = np.random.uniform(low=0.0,high=5.0,size=(2,10000)) # large number of uniform parameter samples for graphing\n",
    "\n",
    "exact_dg = lambda x, y : exact_param_marginals[0](x)*exact_param_marginals[1](y)\n",
    "exact_dg = exact_dg(params_graphing[0,:],params_graphing[1,:])\n",
    "kde_dg = GKDE(params_obs)(params_graphing)\n",
    "\n",
    "# KDEs of true marginals\n",
    "kde_param_marginals = []\n",
    "for i in range(params.shape[0]):\n",
    "        kde_param_marginals.append(GKDE(params_obs[i,:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b505584-d44c-43b9-8c91-f503c27e46e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# constructing and plotting updated marginals\n",
    "\n",
    "x_min = 0.0\n",
    "x_max = 5.0\n",
    "delta = 0.25*(x_max - x_min)\n",
    "x = np.linspace(x_min-delta, x_max+delta, 100)\n",
    "param_labels = [r'$a$', r'$b$']\n",
    "param_str = ['a', 'b']\n",
    "param_marginals = []\n",
    "for i in range(params.shape[0]):\n",
    "    plt.figure()\n",
    "    plt.plot(x, unif_dist(x,[0.0,5.0]), label='Initial', linewidth=2, c=c[0])\n",
    "    param_marginals.append(GKDE(params[i,:], weights=r_vals[0]))\n",
    "    mar = param_marginals[i](x)\n",
    "    plt.plot(x, mar, label = 'Updated', linewidth=4, linestyle='dashed', c=c[1])\n",
    "    plt.plot(x, exact_param_marginals[i](x), label='Data-generating', linewidth=4, linestyle='dotted', c=c[2])\n",
    "    plt.title('Densities for parameter '+param_labels[i])\n",
    "    plt.xlabel(param_labels[i])\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    # fn = 'plots/wave_marginal_' + param_str[i] + '_3.png'\n",
    "    # plt.savefig(fn, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b41c53aa-d428-4d61-9c28-ba43a4873c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# color plot of updated density\n",
    "\n",
    "pi_update = GKDE(params, weights=r_vals[0])(params_graphing)\n",
    "plt.figure()\n",
    "plt.scatter(params_graphing[0,:], params_graphing[1,:], c=pi_update)\n",
    "plt.scatter(params_obs[0,:], params_obs[1,:], c='xkcd:black', s=10, label='data-generating samples')\n",
    "plt.legend()\n",
    "plt.xlabel(param_labels[0])\n",
    "plt.ylabel(param_labels[1])\n",
    "plt.title(f'Color plot of updated density')\n",
    "plt.colorbar(label='density')\n",
    "plt.tight_layout()\n",
    "# plt.savefig('plots/wave_joint_3.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd072719-92c1-47a5-b026-d674361e306e",
   "metadata": {},
   "source": [
    "## Quantifying Differences Between DCI solution and True DG Densities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7fa591d-e7a4-432e-a409-587d847c5207",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating TV metric between updated and exact joint distributions\n",
    "\n",
    "TV = np.abs(pi_update-exact_dg)/2\n",
    "# TV = np.abs(pi_update-kde_dg)/2\n",
    "TV = np.mean(TV)*25\n",
    "print(f'TV metric between pi_update and data-generating joint distribution: {TV}')\n",
    "\n",
    "marginal_TVs = []\n",
    "for i in range(params.shape[0]):\n",
    "    diff = lambda x : np.abs(param_marginals[i](x)-exact_param_marginals[i](x))\n",
    "    # diff = lambda x : np.abs(param_marginals[i](x)-kde_param_marginals[i](x))\n",
    "    TV, _ = quadrature(diff, 0.0, 5.0, tol=1e-2)\n",
    "    marginal_TVs.append(TV/2)\n",
    "print(f'TV metric between pi_update marginals and DG marginals: {marginal_TVs}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
