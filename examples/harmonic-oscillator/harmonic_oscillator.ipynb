{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2019 Steven Mattis and Troy Butler\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import dynamical_systems as ds\n",
    "from scipy.stats import gaussian_kde as GKDE\n",
    "from luq import *\n",
    "\n",
    "import ipywidgets as wd\n",
    "plt.rcParams.update({'font.size': 22})\n",
    "plt.rcParams.update({'axes.linewidth': 2})\n",
    "\n",
    "np.random.seed(123456)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model is for harmonic motion\n",
    "$$y''(t) + 2cy'(t) + \\omega_0^2 y = f(t)$$\n",
    "with damping constant\n",
    "$$c \\in [0.1,1]$$\n",
    "and natural frequency\n",
    "$$\\omega_0\\in[0.5,1]$$\n",
    "and forcing term initially taken to be zero.\n",
    "\n",
    "Note that with the ranges of $c$ and $\\omega_0$ above, it is possible for the system to either be under-, over-, or critically damped (and since $c\\geq 0.1$ it is never undamped, which is almost always physical nonsense). \n",
    "\n",
    "The roots to the characteristic equation are given by\n",
    "$$ r_{1,2} = -c\\pm \\sqrt{c^2-\\omega_0^2}.$$\n",
    "\n",
    "When the system is under-damped, the solution is given by\n",
    "$$ y(t) = e^{-ct}[C_1\\cos(\\omega t) + C_2\\sin(\\omega t)], \\ \\omega=\\sqrt{\\omega_0^2-c^2}. $$\n",
    "\n",
    "\n",
    "When the system is over-damped, the solution is given by \n",
    "$$ y(t) = C_1 e^{r_1t}+C_2 e^{r_2t}. $$\n",
    "\n",
    "And, finally, when the system is critically damped, the solution is given by\n",
    "$$ y(t) = C_1e^{-ct} + C_2 te^{-ct}. $$\n",
    "\n",
    "However, we never expect the system to be critically damped in practice since this is \"too fine-tuned\" of a scenario. \n",
    "\n",
    "The constants $C_1$ and $C_2$ are determined by the initial conditions, which we assume to be given by\n",
    "$$ y(0)=a, y'(0) = b $$\n",
    "where \n",
    "$$ a\\in[1,2] $$ \n",
    "and \n",
    "$$ b\\in[-1,0] $$. \n",
    "\n",
    "In the under-damped case, \n",
    "$$ C_1 = a, \\ \\text{and } \\ C_2 = \\frac{b+ca}{\\omega}. $$\n",
    "\n",
    "In the over-damped case, \n",
    "$$ C_1 = \\frac{b-ar_2}{r_1-r_2}, \\ \\text{and } \\ C_2 = \\frac{b-r_1a}{r_2-r_1} $$\n",
    "\n",
    "A ***true*** distribution of $c, \\omega_0, a$, and $b$ are defined by (non-uniform)\n",
    "Beta distributions and used to generate a set of time series data.\n",
    "\n",
    "An ***initial*** uniform distribution is assumed and updated by the true time series data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uniformly sample the parameter samples to form a \"prediction\" or \"test\" set\n",
    "num_samples = int(2E3)\n",
    "\n",
    "params = np.random.uniform(size=(num_samples, 2))\n",
    "ics = np.random.uniform(size=(num_samples, 2))\n",
    "\n",
    "param_range = np.array([[0.1, 1.0],  # c\n",
    "                        [0.5, 1.0]])  # omega_0\n",
    "ic_range = np.array([[3.0, 3.0],  #a\n",
    "                     [0.0, 0.0]])  #b\n",
    "params = param_range[:, 0] + (param_range[:, 1] - param_range[:, 0]) * params\n",
    "ics = ic_range[:, 0] + (ic_range[:, 1] - ic_range[:, 0]) * ics\n",
    "param_labels = [r'$c$', r'$\\omega_0$']\n",
    "ic_labels = [r'$a$', r'$b$']\n",
    "\n",
    "# Construct the predicted time series data\n",
    "\n",
    "num_time_preds = int(501)  # number of predictions (uniformly space) between [time_start,time_end]\n",
    "time_start = 1.0\n",
    "time_end = 6.0\n",
    "times = np.linspace(time_start, time_end, num_time_preds)\n",
    "\n",
    "phys = ds.HarmonicOscillator()\n",
    "predicted_time_series = phys.solve(ics=ics, params=params, t_eval=times)\n",
    "\n",
    "\n",
    "# Simulate an observed Beta distribution of time series data\n",
    "\n",
    "num_obs = int(3E2)\n",
    "\n",
    "true_a = 2\n",
    "true_b = 2\n",
    "\n",
    "params_obs = np.random.beta(size=(num_obs, 2), a=true_a, b=true_b)\n",
    "ics_obs = np.random.beta(size=(num_obs, 2), a=true_a, b=true_b)\n",
    "params_obs = param_range[:, 0] + (param_range[:, 1] - param_range[:, 0]) * params_obs\n",
    "ics_obs = ic_range[:, 0] + (ic_range[:, 1] - ic_range[:, 0]) * ics_obs\n",
    "\n",
    "observed_time_series = phys.solve(ics=ics_obs, params=params_obs, t_eval=times)\n",
    "\n",
    "# Add noise if desired\n",
    "with_noise = True\n",
    "noise_stdev = 0.25\n",
    "\n",
    "if with_noise:\n",
    "    observed_time_series += noise_stdev * np.random.randn(num_obs, num_time_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use LUQ to learn dynamics and QoIs\n",
    "learn = LUQ(predicted_time_series, observed_time_series, times)\n",
    "\n",
    "# time array indices over which to use\n",
    "time_start_idx = 0\n",
    "time_end_idx = num_time_preds-1\n",
    "\n",
    "num_clean_obs = 16\n",
    "\n",
    "# Clean data with piecewise linear splines\n",
    "learn.clean_data(time_start_idx=time_start_idx, time_end_idx=time_end_idx,\n",
    "                     num_clean_obs=num_clean_obs, tol=5.0e-2, min_knots=3, max_knots=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# learn and classify dynamics\n",
    "# learn.dynamics(cluster_method='gmm', kwargs={'n_components': 3})\n",
    "learn.dynamics(kwargs={'n_clusters': 3, 'n_init': 10})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,8))\n",
    "\n",
    "# chosen_obs = [109, 8]\n",
    "\n",
    "chosen_obs = [0, 8, 10]\n",
    "colors = ['r', 'g', 'b']\n",
    "\n",
    "for i, c in zip(chosen_obs,colors):\n",
    "    plt.plot(learn.times[time_start_idx:time_end_idx], learn.observed_time_series[i,time_start_idx:time_end_idx],\n",
    "             color=c, linestyle='none', marker='.', markersize=10, alpha=0.25)\n",
    "    \n",
    "for i in chosen_obs:\n",
    "    num_i_knots = int(0.5*(2+len(learn.obs_knots[i])))\n",
    "    knots = np.copy(learn.obs_knots[i][num_i_knots:])\n",
    "    knots = np.insert(knots, 0, learn.times[time_start_idx])\n",
    "    knots = np.append(knots, learn.times[time_end_idx])\n",
    "    plt.plot(knots, learn.obs_knots[i][:num_i_knots], 'k', linestyle='dashed', markersize=15, marker='o', linewidth=2)\n",
    "    \n",
    "plt.xlabel('$t$')\n",
    "plt.ylabel('$y(t)$')\n",
    "plt.title('Approximating Dynamics')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,8))\n",
    "\n",
    "for i, c in zip(chosen_obs,colors):\n",
    "    plt.plot(learn.times[time_start_idx:time_end_idx], learn.observed_time_series[i,time_start_idx:time_end_idx],\n",
    "             color=c, linestyle='none', marker='.', markersize=10, alpha=0.25)\n",
    "    \n",
    "for i in chosen_obs:\n",
    "    plt.plot(learn.clean_times, learn.clean_obs[i,:],'k', linestyle='none', marker='s', \n",
    "            markersize=12)\n",
    "    \n",
    "plt.xlabel('$t$')\n",
    "plt.ylabel('$y(t)$')\n",
    "plt.title('Generating Clean Data')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Plot clusters of predicted time series\n",
    "\n",
    "for j in range(learn.num_clusters):\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(24,8), gridspec_kw={'width_ratios': [1, 1]}) \n",
    "    ax1.scatter(np.tile(learn.clean_times,num_samples).reshape(num_samples,num_clean_obs), \n",
    "                learn.clean_predictions, 50, c='gray', marker='.', alpha=0.2)\n",
    "    idx = np.where(learn.predict_labels == j)[0]\n",
    "    ax1.scatter(np.tile(learn.clean_times,len(idx)).reshape(len(idx),num_clean_obs), \n",
    "                learn.clean_predictions[idx,:], 50, c='b', marker='o', alpha=0.2)\n",
    "    ax1.set(title='Cluster ' + str(j+1) + ' in data')\n",
    "    ax1.set_xlabel('$t$')\n",
    "    ax1.set_ylabel('$y(t)$')\n",
    "    \n",
    "    ax2.scatter(params[:,0], params[:,1], 30, c='gray', marker='.', alpha=0.2)\n",
    "    ax2.scatter(params[idx,0], params[idx,1], 50, c='blue', marker='o')\n",
    "    ax2.set(title='Cluster ' + str(j+1) + ' in parameters')\n",
    "    ax2.set_ylabel('$\\omega_0$')\n",
    "    ax2.set_xlabel('$c$')\n",
    "    fig.tight_layout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## # Plot oberved and predicted clusters\n",
    "\n",
    "for j in range(learn.num_clusters):\n",
    "    fig = plt.figure(figsize=(10,8))\n",
    "    plt.scatter(np.tile(learn.clean_times,num_samples).reshape(num_samples,num_clean_obs), \n",
    "                learn.clean_predictions, 10, c='gray', marker='.', alpha=0.2)\n",
    "    idx = np.where(learn.predict_labels == j)[0]\n",
    "    plt.scatter(np.tile(learn.clean_times,len(idx)).reshape(len(idx),num_clean_obs), \n",
    "                learn.clean_predictions[idx,:], 20, c='b', marker='o', alpha=0.3)\n",
    "    idx = np.where(learn.obs_labels == j)[0]    \n",
    "    plt.scatter(np.tile(learn.clean_times,len(idx)).reshape(len(idx),num_clean_obs), \n",
    "                learn.clean_obs[idx, :], 50, c='r', marker='s', alpha=0.2)\n",
    "    plt.title('Classifying cleaned observations')\n",
    "    plt.xlabel('$t$')\n",
    "    plt.ylabel('$y(t)$')\n",
    "    bottom, top = plt.gca().get_ylim()\n",
    "    props = dict(boxstyle='round', facecolor='gray', alpha=0.2)\n",
    "    plt.text(1, (top-bottom)*0.1+bottom, \n",
    "             'Cluster ' + str(j+1), \n",
    "             {'color': 'k', 'fontsize': 20},\n",
    "             bbox=props)\n",
    "    plt.text\n",
    "    fig.tight_layout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find best KPCA transformation for given number of QoI and transform time series data.\n",
    "predict_map, obs_map = learn.learn_qois_and_transform(num_qoi=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%reset -f out\n",
    "\n",
    "def plot_gap(all_eig_vals, n, cluster):\n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "    fig.clear()\n",
    "    #Plotting until maximum number of knots\n",
    "    eig_vals = all_eig_vals[cluster].lambdas_[0:10]\n",
    "    plt.semilogy(np.arange(np.size(eig_vals))+1,eig_vals/np.sum(eig_vals)*100, Marker='.', MarkerSize=20, linestyle='')\n",
    "    plt.semilogy(np.arange(np.size(eig_vals))+1,eig_vals[n]/np.sum(eig_vals)*100*np.ones(np.size(eig_vals)), 'k--')\n",
    "    plt.semilogy(np.arange(np.size(eig_vals))+1,eig_vals[n+1]/np.sum(eig_vals)*100*np.ones(np.size(eig_vals)), 'r--')\n",
    "    plt.text(n+1, eig_vals[n]/np.sum(eig_vals)*150, \n",
    "             r'%2.3f' %(np.sum(eig_vals[0:n+1])/np.sum(eig_vals)*100) + '% of variation explained by first ' + '%1d' %(n+1) + ' PCs.', \n",
    "                                                               {'color': 'k', 'fontsize': 20})\n",
    "    plt.text(n+2, eig_vals[n+1]/np.sum(eig_vals)*150, \n",
    "             r'Order of magnitude of gap is %4.2f.' %(np.log10(eig_vals[n])-np.log10(eig_vals[n+1])), \n",
    "                                                               {'color': 'r', 'fontsize': 20})\n",
    "    s = 'Determining QoI for cluster #%1d' %(cluster+1)\n",
    "    plt.title(s)\n",
    "    plt.xlabel('Principal Component #')\n",
    "    plt.ylabel('% of Variation')\n",
    "    plt.xlim([0.1, np.size(eig_vals)+1])\n",
    "    plt.ylim([0,500])\n",
    "\n",
    "\n",
    "wd.interact(plot_gap, all_eig_vals=wd.fixed(learn.kpcas),\n",
    "            n = wd.IntSlider(value=0, min=0, max=5),\n",
    "            cluster = wd.IntSlider(value=0, min=0, max=learn.num_clusters-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate kernel density estimates on new QoI\n",
    "learn.generate_kdes()\n",
    "# Calculate rejection rates for each cluster and print averages.\n",
    "r_vals = learn.compute_r()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_marginals = []\n",
    "ic_marginals = []\n",
    "true_param_marginals = []\n",
    "true_ic_marginals = []\n",
    "lam_ptr = []\n",
    "cluster_weights = []\n",
    "for i in range(learn.num_clusters):\n",
    "    lam_ptr.append(np.where(learn.predict_labels == i)[0])\n",
    "    cluster_weights.append(len(np.where(learn.obs_labels == i)[0]) / num_obs)\n",
    "\n",
    "for i in range(params.shape[1]):\n",
    "    true_param_marginals.append(GKDE(params_obs[:,i]))\n",
    "    param_marginals.append([])\n",
    "    for j in range(learn.num_clusters):\n",
    "        param_marginals[i].append(GKDE(params[lam_ptr[j], i], weights=learn.r[j]))\n",
    "        \n",
    "# for i in range(ics.shape[1]):\n",
    "#     true_ic_marginals.append(GKDE(ics_obs[:,i]))\n",
    "#     ic_marginals.append([])\n",
    "#     for j in range(learn.num_clusters):\n",
    "#         ic_marginals[i].append(GKDE(ics[lam_ptr[j], i], weights=learn.r[j]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unif_dist(x, p_range):\n",
    "    y = np.zeros(x.shape)\n",
    "    val = 1.0/(p_range[1] - p_range[0])\n",
    "    for i, xi in enumerate(x):\n",
    "        if xi < p_range[0] or xi >  p_range[1]:\n",
    "            y[i] = 0\n",
    "        else:\n",
    "            y[i] = val\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(params.shape[1]):\n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "    fig.clear()\n",
    "    x_min = min(min(params[:, i]), min(params_obs[:, i]))\n",
    "    x_max = max(max(params[:, i]), max(params_obs[:, i]))\n",
    "    delt = 0.25*(x_max - x_min)\n",
    "    x = np.linspace(x_min-delt, x_max+delt, 100)\n",
    "    plt.plot(x, unif_dist(x, param_range[i, :]),\n",
    "         label = 'Initial', linewidth=2)\n",
    "    mar = np.zeros(x.shape)\n",
    "    for j in range(learn.num_clusters):\n",
    "        mar += param_marginals[i][j](x) * cluster_weights[j]\n",
    "    plt.plot(x, mar, label = 'Updated', linewidth=4, linestyle='dashed')\n",
    "    plt.plot(x, true_param_marginals[i](x), label = 'Data-generating', linewidth=4, linestyle='dotted')\n",
    "    plt.title('Densities for parameter ' + param_labels[i], fontsize=20)\n",
    "    plt.legend(fontsize=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(ics.shape[1]):\n",
    "#     fig = plt.figure(figsize=(10,10))\n",
    "#     fig.clear()\n",
    "#     x_min = min(min(ics[:, i]), min(ics_obs[:, i]))\n",
    "#     x_max = max(max(ics[:, i]), max(ics_obs[:, i]))\n",
    "#     delt = 0.25*(x_max - x_min)\n",
    "#     x = np.linspace(x_min-delt, x_max+delt, 100)\n",
    "#     plt.plot(x, unif_dist(x, ic_range[i, :]),\n",
    "#          label = 'Initial')\n",
    "#     mar = np.zeros(x.shape)\n",
    "#     for j in range(learn.num_clusters):\n",
    "#         mar += ic_marginals[i][j](x) * cluster_weights[j]\n",
    "#     plt.plot(x, mar, label = 'Updated', linewidth=4, linestyle='dashed')\n",
    "#     plt.plot(x, true_ic_marginals[i](x), label = 'Data-generating', linewidth=4, linestyle='dotted')\n",
    "#     plt.title('Densities for initial condition ' + ic_labels[i], fontsize=20)\n",
    "#     plt.legend(fontsize=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute TV metric between densities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.integrate.quadrature as quad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def param_init_error(x):\n",
    "    return np.abs(unif_dist(x,param_range[param_num, :])-true_param_marginals[param_num](x))\n",
    "\n",
    "for i in range(params.shape[1]):\n",
    "    param_num=i\n",
    "    TV_metric = quad(param_init_error,param_range[i,0],param_range[i,1],maxiter=1000)\n",
    "    print(TV_metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def param_update_KDE_error(x):\n",
    "    mar = np.zeros(x.shape)\n",
    "    for j in range(learn.num_clusters):\n",
    "        mar += param_marginals[param_num][j](x) * cluster_weights[j]\n",
    "    return np.abs(mar-true_param_marginals[param_num](x))\n",
    "\n",
    "for i in range(params.shape[1]):\n",
    "    param_num=i\n",
    "    TV_metric = quad(param_update_KDE_error,param_range[i,0],param_range[i,1],maxiter=1000)\n",
    "    print(TV_metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import beta "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KDE_error(x):\n",
    "    true_beta = beta(a=true_a, b=true_b,loc=param_range[i,0],scale=param_range[i,1]-param_range[i,0])\n",
    "    return np.abs(true_beta.pdf(x)-true_param_marginals[param_num](x))\n",
    "\n",
    "for i in range(params.shape[1]):\n",
    "    param_num=i\n",
    "    TV_metric = quad(KDE_error,param_range[i,0],param_range[i,1],maxiter=1000)\n",
    "    print(TV_metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute KL divergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def KL_margs_init(x):\n",
    "#     return true_param_marginals[param_num](x)*\\\n",
    "#                 np.log(np.divide(true_param_marginals[param_num](x),\n",
    "#                                  unif_dist(x,param_range[param_num, :])))\n",
    "\n",
    "# for i in range(params.shape[1]):\n",
    "#     param_num=i\n",
    "#     KL = quad(KL_margs,param_range[i,0]-0.1,param_range[i,1]+0.1,maxiter=1000)\n",
    "#     print(KL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def KL_margs_updated(x):\n",
    "#     mar = np.zeros(x.shape)\n",
    "#     for j in range(learn.num_clusters):\n",
    "#         mar += param_marginals[param_num][j](x) * cluster_weights[j]\n",
    "#     return true_param_marginals[param_num](x)*\\\n",
    "#                 np.log(np.divide(true_param_marginals[param_num](x),\n",
    "#                                  mar))\n",
    "\n",
    "# for i in range(params.shape[1]):\n",
    "#     param_num=i\n",
    "#     KL = quad(KL_margs_updated,param_range[i,0]-0.1,param_range[i,1]+0.1,maxiter=1000)\n",
    "#     print(KL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
