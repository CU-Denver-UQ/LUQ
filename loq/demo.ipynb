{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load libraries and setup rejection sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-30T23:06:40.260710Z",
     "start_time": "2019-10-30T23:06:40.118163Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed(123456)\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams.update({'font.size': 14})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-30T23:06:40.293347Z",
     "start_time": "2019-10-30T23:06:40.289585Z"
    }
   },
   "outputs": [],
   "source": [
    "import ipywidgets as wd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-30T23:06:40.615735Z",
     "start_time": "2019-10-30T23:06:40.455028Z"
    }
   },
   "outputs": [],
   "source": [
    "import scipy.stats as sstats \n",
    "from scipy.stats import norm, beta "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-30T23:06:40.832147Z",
     "start_time": "2019-10-30T23:06:40.636725Z"
    }
   },
   "outputs": [],
   "source": [
    "from scipy import optimize ### Use this for curve fitting\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "scaler_1 = StandardScaler()\n",
    "\n",
    "scaler_2 = StandardScaler()\n",
    "\n",
    "scaler_3 = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-30T23:06:40.848347Z",
     "start_time": "2019-10-30T23:06:40.845648Z"
    }
   },
   "outputs": [],
   "source": [
    "from scipy.stats import gaussian_kde as GKDE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-30T23:06:40.984217Z",
     "start_time": "2019-10-30T23:06:40.977049Z"
    }
   },
   "outputs": [],
   "source": [
    "def rejection_sampling(r):\n",
    "    # Perform accept/reject sampling on a set of proposal samples using\n",
    "    # the weights r associated with the set of samples and return\n",
    "    # the indices idx of the proposal sample set that are accepted.\n",
    "    N = r.size # size of proposal sample set\n",
    "    check = np.random.uniform(low=0,high=1,size=N) # create random uniform weights to check r against\n",
    "    M = np.max(r)\n",
    "    new_r = r/M # normalize weights \n",
    "    idx = np.where(new_r>=check)[0] # rejection criterion\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time Series: Problems and Approaches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data and distributions on QoI\n",
    "\n",
    "* A set of time series data typically represents a set of data for different quantities of interest (QoI) since each data point is defined by a different functional. \n",
    "<br>\n",
    "\n",
    "    * A single datum is not enough to define/approximate a distribution for a QoI. But, if data are close enough in time there is a lot of correlation, so can we exploit that to construct distributions on a subset of QoI? \n",
    "   <br> <br>\n",
    "    \n",
    "    * We may take the perspective of a stochastic process and think of the data as coming from a statistical time series from a Markov process. From this perspective, we could try to turn the entire time series of data into a distribution on a single QoI. So, each datum represents the change in the Markov process and actually does come from a single distribution. \n",
    "<br>   \n",
    "\n",
    "    \n",
    "* ***The approaches discussed above both trade many values of data taken at many times for a statement of a distribution at a different time (or perhaps even for a QoI that is time invariant).***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting a time series model\n",
    "\n",
    "* Alternatively, we may consider describing explicitly a stochastic time series model for the given data. \n",
    "From this perspective, we would try to find the best parameters for the time series model. \n",
    "This perspective is related to OED and determining frequency of data collection. \n",
    "<br><br>\n",
    "\n",
    "* A time series model is likely only valid over certain time windows, and we need to define the window of time for which a model is valid, and this will subsequently define a different QoI. For example, over what time do you have a linear model? The collection of time windows defines different regimes for which different linear models are valid. This is related to data assimilation and EKF."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determining time windows for an ODE model\n",
    "\n",
    "* How to determine time windows/regions over which time series models are valid? Investigate. This is a modeling question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Things to investigate and some references\n",
    "\n",
    "Investigate the following:\n",
    "\n",
    "* ROM for time dependent systems and the use of spectral decompositions. \n",
    "\n",
    "* Autoregressive models for time series data (https://en.wikipedia.org/wiki/Autoregressive_model)\n",
    "\n",
    "* Connections to particle filtering and other types of data assimilation? https://en.wikipedia.org/wiki/Particle_filter\n",
    "\n",
    "Some possibly useful references:\n",
    "\n",
    "* http://www.blackarbs.com/blog/time-series-analysis-in-python-linear-models-to-garch/11/1/2016\n",
    "\n",
    "* https://campus.datacamp.com/courses/introduction-to-time-series-analysis-in-python/correlation-and-autocorrelation?ex=1\n",
    "    * https://campus.datacamp.com/courses/introduction-to-time-series-analysis-in-python/autoregressive-ar-models?ex=7\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some initial examples -- Proof of concept"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An ODE model\n",
    "\n",
    "Model is for harmonic motion\n",
    "$$y''(t) + 2cy'(t) + \\omega_0^2 x = f(t)$$\n",
    "with damping constant\n",
    "$$c \\in [0.1,1]$$\n",
    "and natural frequency\n",
    "$$\\omega_0\\in[0.5,2]$$\n",
    "and forcing term initially taken to be zero.\n",
    "\n",
    "Note that with the ranges of $c$ and $\\omega_0$ above, it is possible for the system to either be under-, over-, or critically damped (and since $c\\geq 0.1$ it is never undamped, which is almost always physical nonsense). \n",
    "\n",
    "The roots to the characteristic equation are given by\n",
    "$$ r_1 = -c\\pm \\sqrt{c^2-\\omega_0^2}$$.\n",
    "\n",
    "When the system is under-damped, the solution is given by\n",
    "$$ y(t) = e^{-ct}[C_1\\cos(\\omega t) + C_2\\sin(\\omega t)], \\ \\omega=\\sqrt{\\omega_0^2-c^2}. $$\n",
    "\n",
    "\n",
    "When the system is over-damped, the solution is given by \n",
    "$$ y(t) = C_1 e^{r_1t}+C_2 e^{r_2t}. $$\n",
    "\n",
    "And, finally, when the system is critically damped, the solution is given by\n",
    "$$ y(t) = C_1e^{-ct} + C_2 te^{-ct}. $$\n",
    "\n",
    "However, we never expect the system to be critically damped in practice since this is \"too fine-tuned\" of a scenario. \n",
    "\n",
    "The constants $C_1$ and $C_2$ are determined by the initial conditions, which we assume to be given by\n",
    "$$ y(0)=a, y'(0) = b $$\n",
    "where \n",
    "$$ a\\in[1,2] $$ \n",
    "and \n",
    "$$ b\\in[-1,0] $$. \n",
    "\n",
    "In the under-damped case, \n",
    "$$ C_1 = a, \\ \\text{and } \\ C_2 = \\frac{b+ca}{\\omega}. $$\n",
    "\n",
    "In the over-damped case, \n",
    "$$ C_1 = \\frac{b-ar_2}{r_1-r_2}, \\ \\text{and } \\ C_2 = \\frac{b-r_1a}{r_2-r_1} $$\n",
    "\n",
    "A ***true*** distribution of $c, \\omega_0, a$, and $b$ are defined by (non-uniform) Beta distributions and used to generate a set of time series data.\n",
    "\n",
    "An ***initial*** uniform distribution is assumed and updated by the true time series data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time series data appended with differences and sums\n",
    "\n",
    "We initially assume no errors in the time series data, i.e., the observations are $y(t)$ at some finite set of times $\\{t_i\\}_{i=1}^N$, with $0\\leq t_1 < t_2 < \\cdots < t_N$. \n",
    "\n",
    "We also take differences and summations of the time series (to extract derivative and integral type information) and append to the data to determine if new/dominant features (i.e., principal components) are found. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-30T23:06:43.172235Z",
     "start_time": "2019-10-30T23:06:43.158172Z"
    }
   },
   "outputs": [],
   "source": [
    "def y(t, c, omega_0, a, b):\n",
    "    z = np.zeros(len(c))\n",
    "    ind_under = np.where(np.greater(omega_0, c))[0]\n",
    "    ind_over = np.where(np.greater(c, omega_0))[0]\n",
    "    # First solve for the under-damped case\n",
    "    if ind_under.size > 0:\n",
    "        omega = np.sqrt(omega_0[ind_under]**2 - c[ind_under]**2)\n",
    "        C_1 = a[ind_under]\n",
    "        C_2 = (b[ind_under]+c[ind_under]*a[ind_under])/omega\n",
    "        \n",
    "        z[ind_under] = np.exp(-c[ind_under]*t) * (C_1*np.cos(omega*t)\n",
    "                                                  + C_2*np.sin(omega*t))\n",
    "        \n",
    "    if ind_over.size > 0:\n",
    "        r_1 = -c[ind_over] - np.sqrt(c[ind_over]**2 - omega_0[ind_over]**2)\n",
    "        r_2 = -c[ind_over] + np.sqrt(c[ind_over]**2 - omega_0[ind_over]**2)\n",
    "        C_1 = (b[ind_over]-a[ind_over]*r_2)/(r_1-r_2)\n",
    "        C_2 = (b[ind_over]-r_1*a[ind_over])/(r_2-r_1)\n",
    "        \n",
    "        z[ind_over] = C_1*np.exp(r_1*t) + C_2*np.exp(r_2*t)\n",
    "        \n",
    "    return z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate a test/prediction/prior set of parameters and observed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-30T23:06:44.199272Z",
     "start_time": "2019-10-30T23:06:44.190156Z"
    }
   },
   "outputs": [],
   "source": [
    "# Uniformly sample the parameter samples to form a \"prediction\" or \"test\" set\n",
    "\n",
    "num_samples = int(1E3)\n",
    "\n",
    "lam = np.random.uniform(size=(num_samples,4))\n",
    "\n",
    "lam[:,0] = 0.1 + 0.4*lam[:,0]  #c\n",
    "lam[:,1] = 0.5 + 1.5*lam[:,1] #omega_0\n",
    "lam[:,2] = 1 + 1*lam[:,2] #a\n",
    "lam[:,3] = -1 + 1*lam[:,3]   #b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-30T23:06:44.509345Z",
     "start_time": "2019-10-30T23:06:44.492852Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Construct the predicted time series data\n",
    "\n",
    "num_time_obs = int(50) #number of observations (uniformly space) between [time_start,time_end]\n",
    "time_start = 0.5\n",
    "time_end = 3.5\n",
    "times = np.linspace(time_start, time_end, num_time_obs)\n",
    "\n",
    "Q_samples_TS = np.zeros((num_samples,num_time_obs))\n",
    "\n",
    "for i in range(num_time_obs):\n",
    "    Q_samples_TS[:,i] = y(times[i],lam[:,0],lam[:,1],lam[:,2],lam[:,3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate an observed set of data from a different distribution on parameters\n",
    "\n",
    "The idea here is to show that we can reconstruct/recover a \"true\" distribution on parameters from observations. \n",
    "This establishes that we are indeed ***inverting*** a distribution on outputs.\n",
    "\n",
    "However, in practice, we may only observe a ***single*** time series of data, polluted by noise, and impose a distribution on this time series data to invert.\n",
    "\n",
    "Below, we simulate a peaked Beta distribution on a subset of the whole parameter space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-30T23:06:45.406697Z",
     "start_time": "2019-10-30T23:06:45.397818Z"
    }
   },
   "outputs": [],
   "source": [
    "# Simulate an observed distribution of time series data\n",
    "\n",
    "num_obs_samples = int(1E3)\n",
    "\n",
    "true_a = 2\n",
    "true_b = 2\n",
    "\n",
    "lam_obs = np.random.beta(size=(num_obs_samples,4),a=true_a,b=true_b)\n",
    "\n",
    "lam_obs[:,0] = 0.1 + 0.4*lam_obs[:,0]  #c\n",
    "lam_obs[:,1] = 0.5 + 1.5*lam_obs[:,1]  #omega_0\n",
    "lam_obs[:,2] = 1 + 1*lam_obs[:,2]   #a\n",
    "lam_obs[:,3] = -1 + 1*lam_obs[:,3]   #b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-30T23:06:46.583455Z",
     "start_time": "2019-10-30T23:06:46.566414Z"
    }
   },
   "outputs": [],
   "source": [
    "Q_obs_samples_TS = np.zeros((num_obs_samples,num_time_obs))\n",
    "\n",
    "with_noise = False\n",
    "noise_stdev = 0.05\n",
    "\n",
    "if with_noise:\n",
    "    for i in range(num_time_obs):\n",
    "        Q_obs_samples_TS[:,i] = y(times[i],lam_obs[:,0],lam_obs[:,1],lam_obs[:,2],lam_obs[:,3]) +\\\n",
    "                                 noise_stdev*np.random.randn(num_obs_samples)\n",
    "else:\n",
    "    for i in range(num_time_obs):\n",
    "        Q_obs_samples_TS[:,i] = y(times[i],lam_obs[:,0],lam_obs[:,1],lam_obs[:,2],lam_obs[:,3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot the predicted time series data in blue and the observed data in red"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-30T23:06:47.943047Z",
     "start_time": "2019-10-30T23:06:47.491509Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "\n",
    "for i in range(0,num_samples,20):\n",
    "    plt.plot(times,Q_samples_TS[i,0:num_time_obs],'bo')\n",
    "    \n",
    "for i in range(0,num_obs_samples,10):\n",
    "    plt.plot(times,Q_obs_samples_TS[i,0:num_time_obs],'r*')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determining some QoI through L2 style projection to splines with N knots followed by PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-30T23:06:48.861668Z",
     "start_time": "2019-10-30T23:06:48.856794Z"
    }
   },
   "outputs": [],
   "source": [
    "from loq import *\n",
    "learn = LoQ(Q_samples_TS, Q_obs_samples_TS, times, times)\n",
    "learn.clean_data(time_start=0.5, time_end=3.5, num_time_obs=50, rel_tol=1.0e-3, min_knots=5, max_knots=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-30T23:07:17.239524Z",
     "start_time": "2019-10-30T23:07:13.335018Z"
    }
   },
   "outputs": [],
   "source": [
    "# N = 7\n",
    "\n",
    "# knots_init = np.linspace(times[0],times[-1], N)[1:-1]\n",
    "\n",
    "# q_predict_pl = np.zeros((num_samples,2*N-2))\n",
    "\n",
    "# for i in range(num_samples):\n",
    "#     q_predict_pl[i,:], _ = optimize.curve_fit(lambda x, *params_0: wrapper_fit_func(x, N, params_0), \n",
    "#                                               times, Q_samples_TS[i,:], \n",
    "#                                               p0=np.hstack([np.zeros(N),knots_init]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-30T23:08:23.393968Z",
     "start_time": "2019-10-30T23:08:23.213799Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "\n",
    "for i in range(0,num_samples,200):\n",
    "    plt.plot(learn.surrogate_times,learn.surrogate_predictions[i,:])\n",
    "    plt.plot(times,Q_samples_TS[i,0:num_time_obs],'bo')\n",
    "    #plt.plot(times, piecewise_linear(times, q_predict_pl[i,N:2*N], q_predict_pl[i,0:N]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-30T23:08:25.712794Z",
     "start_time": "2019-10-30T23:08:25.706944Z"
    }
   },
   "outputs": [],
   "source": [
    "# print(q_predict_pl[0,:])\n",
    "# print(q_predict_pl[0,N:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-30T23:08:26.424125Z",
     "start_time": "2019-10-30T23:08:26.290596Z"
    }
   },
   "outputs": [],
   "source": [
    "# i = 3\n",
    "# plt.plot(times,Q_samples_TS[i,0:num_time_obs],'bo')\n",
    "# plt.plot(times, piecewise_linear(times, q_predict_pl[i,N:], q_predict_pl[i,0:N]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-30T23:08:26.749749Z",
     "start_time": "2019-10-30T23:08:26.714670Z"
    }
   },
   "outputs": [],
   "source": [
    "# num_surrogate_time_obs = num_time_obs\n",
    "\n",
    "# surrogate_times = np.linspace(time_start, time_end, num_surrogate_time_obs)\n",
    "\n",
    "# surrogate_predictions = np.zeros((num_samples,num_surrogate_time_obs))\n",
    "# for i in range(num_samples):\n",
    "#     surrogate_predictions[i,:] = piecewise_linear(surrogate_times, \n",
    "#                                              q_predict_pl[i,N:2*N], \n",
    "#                                              q_predict_pl[i,0:N])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-30T23:08:27.465257Z",
     "start_time": "2019-10-30T23:08:27.338650Z"
    }
   },
   "outputs": [],
   "source": [
    "# i = 3\n",
    "# plt.plot(times,Q_samples_TS[i,0:num_time_obs],'bo')\n",
    "# plt.plot(times, piecewise_linear(times, q_predict_pl[i,N:], q_predict_pl[i,0:N]))\n",
    "# plt.plot(surrogate_times, surrogate_predictions[i,:],'rs')\n",
    "plt.figure(figsize=(10,5))\n",
    "\n",
    "for i in range(0,num_samples,200):\n",
    "    plt.plot(learn.surrogate_times,learn.surrogate_obs[i,:])\n",
    "    plt.plot(times,Q_obs_samples_TS[i,0:num_time_obs],'bo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-30T23:26:55.789372Z",
     "start_time": "2019-10-30T23:26:55.620558Z"
    }
   },
   "outputs": [],
   "source": [
    "kwargs = [{'n_clusters': 3,\n",
    "           'n_init': 10},\n",
    "          {'n_clusters': 3,\n",
    "           'assign_labels': 'discretize',\n",
    "           'random_state': 0}]\n",
    "\n",
    "learn.learn_dynamics(cluster_methods=['kmeans', 'spectral'], kwargs=kwargs)\n",
    "\n",
    "learn.classify_dynamics(kernel=\"rbf\")\n",
    "\n",
    "learn.learn_qoi(kernel='linear')\n",
    "\n",
    "# from sklearn.cluster import SpectralClustering\n",
    "# from sklearn.cluster import KMeans\n",
    "\n",
    "# kmeans = False\n",
    "\n",
    "# k_means = KMeans(init='k-means++', n_clusters=3, n_init=10)\n",
    "# k_means.fit(surrogate_predictions)\n",
    "\n",
    "# predict_labels_kmeans = k_means.labels_\n",
    "\n",
    "# from sklearn import svm\n",
    "# clustering = SpectralClustering(n_clusters=3,\n",
    "#              assign_labels=\"discretize\",\n",
    "#              random_state=0).fit(surrogate_predictions)\n",
    "# kernel = \"rbf\" # \"rbf\", \"poly\", \"sigmoid\", \"linear\"\n",
    "# clf = svm.SVC(kernel=kernel, gamma='auto')\n",
    "# clf.fit(surrogate_predictions, clustering.labels_)\n",
    "# # clf.fit(surrogate_predictions, k_means.labels_)\n",
    "# predict_labels_clustering = clf.predict(surrogate_predictions)\n",
    "# #print(clf.score(surrogate_predictions, clustering.labels_))\n",
    "# if kmeans:\n",
    "#     predict_labels = predict_labels_kmeans\n",
    "# else:\n",
    "#     predict_labels = predict_labels_clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-30T23:26:55.974087Z",
     "start_time": "2019-10-30T23:26:55.963569Z"
    }
   },
   "outputs": [],
   "source": [
    "# print(predict_labels_clustering)\n",
    "# print(k_means.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-30T23:22:52.102775Z",
     "start_time": "2019-10-30T23:22:51.044946Z"
    }
   },
   "outputs": [],
   "source": [
    "for i in range(num_samples):\n",
    "    if predict_labels[i]==2:\n",
    "        plt.plot(surrogate_times, surrogate_predictions[i,:])\n",
    "\n",
    "# time_idx = np.where(clustering.labels_==0)[0]\n",
    "# for i in range(num_samples):   \n",
    "#     plt.plot(surrogate_times[time_idx], surrogate_predictions[i,time_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-30T23:10:50.246608Z",
     "start_time": "2019-10-30T23:10:50.189023Z"
    }
   },
   "outputs": [],
   "source": [
    "# from sklearn.decomposition import PCA, KernelPCA\n",
    "\n",
    "# X_std_1 = scaler_1.fit_transform(surrogate_predictions[np.where(predict_labels==0)[0],:])\n",
    "\n",
    "# X_std_2 = scaler_2.fit_transform(surrogate_predictions[np.where(predict_labels==1)[0],:])\n",
    "\n",
    "# X_std_3 = scaler_3.fit_transform(surrogate_predictions[np.where(predict_labels==2)[0],:])                             \n",
    "\n",
    "# kernel = \"linear\" #\"rbf\" # \"linear\", \"poly\", \"sigmoid\", \"cosine\"\n",
    "\n",
    "# kpca1 = KernelPCA(kernel=kernel, fit_inverse_transform=False)\n",
    "# kpca2 = KernelPCA(kernel=kernel, fit_inverse_transform=False)\n",
    "# kpca3 = KernelPCA(kernel=kernel, fit_inverse_transform=False)\n",
    "\n",
    "# X_kpca1 = kpca1.fit_transform(X_std_1)\n",
    "# X_kpca2 = kpca2.fit_transform(X_std_2)\n",
    "# X_kpca3 = kpca3.fit_transform(X_std_3)\n",
    "\n",
    "\n",
    "# eig_vals_1 = kpca1.lambdas_\n",
    "# eig_vals_2 = kpca2.lambdas_\n",
    "# eig_vals_3 = kpca3.lambdas_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-30T23:10:51.735968Z",
     "start_time": "2019-10-30T23:10:51.173944Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%reset -f out\n",
    "\n",
    "def plot_gap(eig_vals, n):\n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "    fig.clear()\n",
    "    plt.semilogy(np.arange(np.size(eig_vals)),eig_vals, Marker='*', linestyle='')\n",
    "    plt.semilogy(np.arange(np.size(eig_vals)),eig_vals[n]*np.ones(np.size(eig_vals)), 'r--')\n",
    "    plt.semilogy(np.arange(np.size(eig_vals)),eig_vals[n+1]*np.ones(np.size(eig_vals)), 'r--')\n",
    "    s = 'Orders of magnitude between %d and %d e.vals is %4.2f' %(n, n+1, \n",
    "                                                                  np.log10(eig_vals[n])-np.log10(eig_vals[n+1]))\n",
    "    s += '\\n and e.vals up to %d account for %4.2f percent of variation' %(n, \n",
    "                                                                           np.sum(eig_vals[0:n+1])/np.sum(eig_vals)*100)\n",
    "    plt.title(s)\n",
    "\n",
    "wd.interact(plot_gap,\n",
    "            eig_vals = wd.fixed(eig_vals_2),\n",
    "            n = wd.IntSlider(value=0, min=0, max=20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-30T18:49:39.436030Z",
     "start_time": "2019-10-30T18:49:39.433007Z"
    }
   },
   "outputs": [],
   "source": [
    "# %reset -f out\n",
    "\n",
    "# def plot_eigvecs(eig_vals, eig_vecs, n):\n",
    "#     fig = plt.figure(figsize=(10,10))\n",
    "#     fig.clear()\n",
    "#     plt.plot(np.arange(np.size(eig_vals)),eig_vecs[:,n], Marker='*', linestyle='', color='k')\n",
    "#     plt.title('Eigenvector ' + str(n))\n",
    "#     plt.ylim([-1,1])\n",
    "\n",
    "# wd.interact(plot_eigvecs, \n",
    "#             eig_vals = wd.fixed(eig_vals_1),\n",
    "#             eig_vecs = wd.fixed(eig_vecs_1),\n",
    "#             n = wd.IntSlider(value=0,min=0,max=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-30T23:11:21.962968Z",
     "start_time": "2019-10-30T23:11:21.701167Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Clear figure each time this is run to save memory\n",
    "%reset -f out\n",
    "\n",
    "#define all the potential QoI\n",
    "q_predict_PCA_1 = X_kpca1 #np.dot(X_std_1, eig_vecs_1)\n",
    "q_predict_PCA_2 = X_kpca2#np.dot(X_std_2, eig_vecs_2)\n",
    "q_predict_PCA_3 = X_kpca3 #np.dot(X_std_3, eig_vecs_3)\n",
    "\n",
    "#create a plot function to check out any pairs of QoI desired\n",
    "def plot_QoI_pairs(q_predict, q1, q2):\n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "    fig.clear()\n",
    "    plt.scatter(q_predict[:,q1],q_predict[:,q2])\n",
    "    plt.title('Visualizing the skewness/correlation between QoI ' + str(q1) + ' and ' + str(q2))\n",
    "\n",
    "#allow interactivity of plotting\n",
    "wd.interact(plot_QoI_pairs, \n",
    "            q_predict = wd.fixed(q_predict_PCA_1), \n",
    "            q1 = wd.IntSlider(value=0,min=0,max=q_predict_PCA_1[0,:].size-1,step=1), \n",
    "            q2 = wd.IntSlider(value=1,min=0,max=q_predict_PCA_1[0,:].size-1,step=1),\n",
    "            continuous_update=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now choose how many QoI to use from the principal components and construct a predicted density either using a standard Gaussian KDE \n",
    "\n",
    "***Still to-do:*** Try just using machine learning to determine an optimal KDE with a chosen kernel that allows for compact support and uses cross-validation to optimize bandwidth. Need to investigate further. Code below is commented out because it takes too long to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-30T23:11:25.220973Z",
     "start_time": "2019-10-30T23:11:25.209533Z"
    }
   },
   "outputs": [],
   "source": [
    "QoI_list = range(4)\n",
    "QoI_num = len(QoI_list)\n",
    "\n",
    "q_predict_maps = [q_predict_PCA_1[:,QoI_list], q_predict_PCA_2[:,QoI_list], q_predict_PCA_3[:,QoI_list]]\n",
    "\n",
    "pi_Q_kdes = [GKDE( q_predict_maps[0].T ), GKDE( q_predict_maps[1].T ), GKDE( q_predict_maps[2].T )]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now transform the observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-30T23:11:29.523095Z",
     "start_time": "2019-10-30T23:11:25.222712Z"
    }
   },
   "outputs": [],
   "source": [
    "q_obs_pl = np.zeros((num_obs_samples,2*N-2))\n",
    "\n",
    "p0_init = np.hstack([np.zeros(N), knots_init])\n",
    "p0_init[0] = 1\n",
    "\n",
    "for i in range(num_obs_samples):\n",
    "    q_obs_pl[i,:], _ = optimize.curve_fit(lambda x, *params_0: wrapper_fit_func(x, N, params_0), \n",
    "                                          times, Q_obs_samples_TS[i,:], \n",
    "                                          p0=p0_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-30T23:11:29.558533Z",
     "start_time": "2019-10-30T23:11:29.524662Z"
    }
   },
   "outputs": [],
   "source": [
    "surrogate_obs = np.zeros((num_obs_samples,num_surrogate_time_obs))\n",
    "for i in range(num_samples):\n",
    "    surrogate_obs[i,:] = piecewise_linear(surrogate_times, \n",
    "                                          q_obs_pl[i,N:2*N], \n",
    "                                          q_obs_pl[i,0:N])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-30T23:11:29.579976Z",
     "start_time": "2019-10-30T23:11:29.560313Z"
    }
   },
   "outputs": [],
   "source": [
    "# obs_labels = k_means.predict(surrogate_obs)\n",
    "if kmeans:\n",
    "    obs_labels = k_means.predict(surrogate_obs)\n",
    "else:\n",
    "    obs_labels = clf.predict(surrogate_obs) #clustering.transform(surrogate_obs)\n",
    "print(obs_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-30T23:12:20.842489Z",
     "start_time": "2019-10-30T23:12:19.207845Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "\n",
    "cluster_num = 0\n",
    "for i in range(num_samples):\n",
    "    if predict_labels[i]==cluster_num:\n",
    "        plt.plot(surrogate_times, surrogate_predictions[i,:],'b*')\n",
    "\n",
    "for i in range(num_obs_samples): \n",
    "    if obs_labels[i]==cluster_num:\n",
    "        plt.plot(surrogate_times, surrogate_obs[i,:],'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-30T23:11:31.283482Z",
     "start_time": "2019-10-30T23:11:31.259288Z"
    }
   },
   "outputs": [],
   "source": [
    "X_test_1 = scaler_1.transform(surrogate_obs[np.where(obs_labels==0)[0],:])\n",
    "\n",
    "X_test_2 = scaler_2.transform(surrogate_obs[np.where(obs_labels==1)[0],:])\n",
    "\n",
    "X_test_3 = scaler_3.transform(surrogate_obs[np.where(obs_labels==2)[0],:])                             \n",
    "\n",
    "q_obs_PCA_1 = kpca1.transform(X_test_1)#np.dot(X_test_1, eig_vecs_1)\n",
    "\n",
    "q_obs_PCA_2 = kpca2.transform(X_test_2)#np.dot(X_test_2, eig_vecs_2)\n",
    "\n",
    "q_obs_PCA_3 = kpca3.transform(X_test_3)#np.dot(X_test_3, eig_vecs_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-30T23:11:31.551911Z",
     "start_time": "2019-10-30T23:11:31.284819Z"
    }
   },
   "outputs": [],
   "source": [
    "# Visually inspect results\n",
    "#Clear figure each time this is run to save memory\n",
    "%reset -f out\n",
    "\n",
    "#create a plot function to check out any pairs of QoI desired\n",
    "def plot_QoI_pairs(q_predict, q_obs, q1, q2):\n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "    fig.clear()\n",
    "    plt.scatter(q_predict[:,q1],q_predict[:,q2])\n",
    "    plt.scatter(q_obs[:,q1],q_obs[:,q2])\n",
    "\n",
    "#allow interactivity of plotting\n",
    "wd.interact(plot_QoI_pairs, \n",
    "            q_predict = wd.fixed(q_predict_PCA_2), \n",
    "            q_obs = wd.fixed(q_obs_PCA_2),\n",
    "            q1 = wd.IntSlider(value=0,min=0,max=q_predict_PCA_2[0,:].size-1,step=1), \n",
    "            q2 = wd.IntSlider(value=1,min=0,max=q_predict_PCA_2[0,:].size-1,step=1),\n",
    "            continuous_update=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now extract the same number of QoI from the observed set and construct an observed density."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-30T23:11:31.558536Z",
     "start_time": "2019-10-30T23:11:31.553713Z"
    }
   },
   "outputs": [],
   "source": [
    "q_obs_maps = [q_obs_PCA_1[:,QoI_list], q_obs_PCA_2[:,QoI_list], q_obs_PCA_3[:,QoI_list]]\n",
    "\n",
    "pi_obs_kdes = [GKDE( q_obs_maps[0].T ), GKDE( q_obs_maps[1].T ), GKDE( q_obs_maps[2].T )]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now perform rejection sampling on predicted set to obtain an i.i.d. sample set from a distribution consistent with the observed (consistent in the sense that it comes from a pullback measure).\n",
    "\n",
    "We printout the acceptance rate of samples as well as the mean value of the rejection ratio \n",
    "\n",
    "$$\n",
    "    r(\\lambda) = \\frac{\\pi_{\\mathcal{D}}^{obs}(Q(\\lambda))}{\\pi_\\mathcal{D}^{predict}(Q(\\lambda))}, \n",
    "$$\n",
    "\n",
    "which should be ***close*** to 1 to indicate that the updated density defined by\n",
    "\n",
    "$$\n",
    "    \\pi_\\Lambda^{update} = \\pi_\\Lambda^{predict}(\\lambda)\\frac{\\pi_{\\mathcal{D}}^{obs}(Q(\\lambda))}{\\pi_\\mathcal{D}^{predict}(Q(\\lambda))}\n",
    "$$\n",
    "\n",
    "is in fact a density. This form and result follows from the disintegration theorem. See [Combining Push-Forward Measures and Bayes' Rule to Construct Consistent Solutions to Stochastic Inverse Problems](https://epubs.siam.org/doi/abs/10.1137/16M1087229) for more details (note that I am using notation more consistent with the newer reference on this subject found [here](https://epubs.siam.org/doi/abs/10.1137/18M1181675) on convergence properties of this updated density using approximate QoI maps)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-30T23:11:31.632479Z",
     "start_time": "2019-10-30T23:11:31.560799Z"
    }
   },
   "outputs": [],
   "source": [
    "num_clusters = 3\n",
    "\n",
    "r = []\n",
    "samples_to_keep = []\n",
    "consistent_lam = []\n",
    "\n",
    "for i in range(num_clusters):\n",
    "#First compute the rejection ratio\n",
    "    r.append(np.divide(pi_obs_kdes[i](q_predict_maps[i].T),pi_Q_kdes[i](q_predict_maps[i].T)))\n",
    "\n",
    "    #Now perform rejection sampling and return the indices we keep\n",
    "    samples_to_keep.append(rejection_sampling(r[i]))\n",
    "\n",
    "    #Now print some diagnostics\n",
    "    print(r[i].mean()) # Want this close to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-30T23:11:31.638036Z",
     "start_time": "2019-10-30T23:11:31.633845Z"
    }
   },
   "outputs": [],
   "source": [
    "# # Visually inspect results\n",
    "# #Clear figure each time this is run to save memory\n",
    "# %reset -f out\n",
    "\n",
    "# #create a plot function to check out any pairs of lambda desired\n",
    "# def plot_lambda_pairs(lam1, lam2):\n",
    "#     fig = plt.figure(figsize=(10,10))\n",
    "#     fig.clear()\n",
    "#     plt.scatter(lam[:,lam1],lam[:,lam2])\n",
    "#     plt.scatter(lam_obs[:,lam1],lam_obs[:,lam2])\n",
    "#     plt.scatter(consistent_lam[:,lam1],consistent_lam[:,lam2])\n",
    "\n",
    "# #allow interactivity of plotting\n",
    "# wd.interact(plot_lambda_pairs, \n",
    "#             lam1 = wd.IntSlider(value=0,min=0,max=3,step=1), \n",
    "#             lam2 = wd.IntSlider(value=1,min=0,max=3,step=1),\n",
    "#             continuous_update=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimate marginals of parameters using a weighted KDE fit. This requires `scipy` to be version 1.2.1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-30T23:11:31.655233Z",
     "start_time": "2019-10-30T23:11:31.640797Z"
    }
   },
   "outputs": [],
   "source": [
    "param_marginals = []\n",
    "true_sample_marginals = []\n",
    "\n",
    "lam_ptr = [np.where(predict_labels==0)[0],\n",
    "           np.where(predict_labels==1)[0],\n",
    "           np.where(predict_labels==2)[0]]\n",
    "\n",
    "cluster_weights = [len(np.where(obs_labels==0)[0]) / num_obs_samples,\n",
    "                   len(np.where(obs_labels==1)[0]) / num_obs_samples,\n",
    "                   len(np.where(obs_labels==2)[0]) / num_obs_samples] \n",
    "\n",
    "for i in range(4):\n",
    "    true_sample_marginals.append(GKDE(lam_obs[:,i]))\n",
    "    param_marginals.append([])\n",
    "    for j in range(num_clusters):\n",
    "        param_marginals[i].append(GKDE(lam[lam_ptr[j],i], weights=r[j]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-30T23:11:31.866238Z",
     "start_time": "2019-10-30T23:11:31.656539Z"
    }
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,10))\n",
    "fig.clear()\n",
    "x = np.linspace(0.1,1,100)\n",
    "plt.plot(x, 1/.9*np.ones(100), label = 'Initial guess')\n",
    "plt.plot(x, param_marginals[0][0](x) * cluster_weights[0] \n",
    "           +param_marginals[0][1](x) * cluster_weights[1]\n",
    "           +param_marginals[0][2](x) * cluster_weights[2], label = 'Estimated pullback')\n",
    "plt.plot(x, true_sample_marginals[0](x), label = 'Actual density')\n",
    "plt.title('Comparing pullback to actual density of parameter $c$', fontsize=16)\n",
    "plt.legend(fontsize=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-30T23:11:32.070512Z",
     "start_time": "2019-10-30T23:11:31.867560Z"
    }
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,10))\n",
    "fig.clear()\n",
    "x = np.linspace(0.5,2,100)\n",
    "plt.plot(x, 1/1.5*np.ones(100), label = 'Initial guess')\n",
    "plt.plot(x, param_marginals[1][0](x) * cluster_weights[0] \n",
    "           +param_marginals[1][1](x) * cluster_weights[1]\n",
    "           +param_marginals[1][2](x) * cluster_weights[2], label = 'Estimated pullback')\n",
    "plt.plot(x, true_sample_marginals[1](x), label = 'Actual density')\n",
    "plt.title('Comparing pullback to actual density of parameter $\\omega_0$', fontsize=16)\n",
    "plt.legend(fontsize=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-30T23:11:32.275613Z",
     "start_time": "2019-10-30T23:11:32.071831Z"
    }
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,10))\n",
    "fig.clear()\n",
    "x = np.linspace(1,2,100)\n",
    "plt.plot(x, 1*np.ones(100), label = 'Initial guess')\n",
    "plt.plot(x, param_marginals[2][0](x) * cluster_weights[0] \n",
    "           +param_marginals[2][1](x) * cluster_weights[1]\n",
    "           +param_marginals[2][2](x) * cluster_weights[2], label = 'Estimated pullback')\n",
    "plt.plot(x, true_sample_marginals[2](x), label = 'Actual density')\n",
    "plt.title('Comparing pullback to actual density of parameter $a$', fontsize=16)\n",
    "plt.legend(fontsize=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-30T23:11:32.487032Z",
     "start_time": "2019-10-30T23:11:32.276973Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,10))\n",
    "fig.clear()\n",
    "x = np.linspace(-1,0,100)\n",
    "plt.plot(x, 1*np.ones(100), label = 'Initial guess')\n",
    "plt.plot(x, param_marginals[3][0](x) * cluster_weights[0] \n",
    "           +param_marginals[3][1](x) * cluster_weights[1]\n",
    "           +param_marginals[3][2](x) * cluster_weights[2], label = 'Estimated pullback')\n",
    "plt.plot(x, true_sample_marginals[3](x), label = 'Actual density')\n",
    "plt.title('Comparing pullback to actual density of parameter $b$', fontsize=16)\n",
    "plt.legend(fontsize=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-30T23:11:32.636176Z",
     "start_time": "2019-10-30T23:11:32.488439Z"
    }
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,10))\n",
    "fig.clear()\n",
    "\n",
    "plt.scatter(lam[lam_ptr[0],3],q_predict_PCA_1[:,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workflow of BET + LoQ\n",
    "\n",
    "BET.solve_TS_data(predicted_time_series_array, observed_time_series_array)\n",
    "\n",
    "### User does not see the following calls\n",
    "\n",
    "LoQ.clean_data()\n",
    "\n",
    "LoQ.learn_dynamics <--- labeling\n",
    "\n",
    "LoQ.classify_dynamics <--- svm\n",
    "\n",
    "LoQ.learn_QoI <---- kPCA\n",
    "\n",
    "LoQ.classify_observations <---- svm to label observations\n",
    "LoQ.transform_observations <--- kPCA on labeled oversations\n",
    "\n",
    "LoQ.create_sample_objects <---- output sample objects for predicted and observed QoI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For non-time series (i.e., for spatial) data\n",
    "\n",
    "BET.solve_spatial_data(spatial_data_array, observed_time_series_array)\n",
    "\n",
    "No call to LoQ.clean_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
