{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-03T16:27:00.082881Z",
     "start_time": "2019-12-03T16:27:00.066756Z"
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "## Uncomment this cell when giving in presentation mode\n",
    "## Presentation mode details: Use \"classic\" jupyter notebook view and make\n",
    "## sure to have the RISE extension installed.\n",
    "\n",
    "# from IPython.display import HTML\n",
    "\n",
    "# HTML('''<script>\n",
    "# code_show=true; \n",
    "# function code_toggle() {\n",
    "# if (code_show){\n",
    "# $('div.input').hide();\n",
    "# } else {\n",
    "# $('div.input').show();\n",
    "# }\n",
    "# code_show = !code_show\n",
    "# } \n",
    "# $( document ).ready(code_toggle);\n",
    "# </script>\n",
    "# <form action=\"javascript:code_toggle()\"><input type=\"submit\" value=\"This documment is written in a Jupyter notebook and the code used to produce these analyses has been hidden for ease of reading.  \n",
    "# To make the code visiable click here\"></form>''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### <center> What we need from Python: The Libraries\n",
    "___\n",
    "\n",
    " - `numpy` to handle sample sets\n",
    " \n",
    "    - `random` subpackage to generate random samples of various distributions.\n",
    "\n",
    " - `scipy.stats`\n",
    "\n",
    "    - A standard kernel density estimator to approximate densities on sample sets.\n",
    "\n",
    "    - Evaluate some standard density functions (e.g., normal) on sample sets.\n",
    "\n",
    " - `matplotlib.pyplot` to visualize results.\n",
    "    \n",
    " - [`LUQ`](https://github.com/CU-Denver-UQ/LUQ) - But we will just use some `sklearn` packages directly in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# The libraries we will use for part 1 of talk\n",
    "import numpy as np\n",
    "from scipy.stats import norm, uniform # The standard Normal distribution\n",
    "from scipy.stats import gaussian_kde as kde # A standard kernel density estimator\n",
    "import scipy.io as sio \n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA, KernelPCA\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import cm\n",
    "plt.rcParams.update({'figure.max_open_warning': 0})   #remove warning of so many open figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# For more interactive visualizations (not always used in every presentation)\n",
    "%matplotlib notebook\n",
    "from scipy.optimize import bisect #Used much later -- don't worry about it\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets\n",
    "\n",
    "from IPython.display import Markdown, display\n",
    "def printmd(string):\n",
    "    display(Markdown(string))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <center> Learning Uncertain Quantities for Data-Consistent Inversion: A Data-to-Distribution Pipeline</center>\n",
    "___\n",
    "    \n",
    "<font color='blue'><center>**T. Butler**</center></font>\n",
    "<center>Co-authors: S. Mattis, K. Steffen, C. Dawson, and D. Estep</center></font>\n",
    "    <br>\n",
    "    <center>University of Colorado Denver\n",
    "    <br>\n",
    "    troy.butler@ucdenver.edu</center>\n",
    "\n",
    "___\n",
    "\n",
    "    \n",
    "[***Learning Quantities of Interest from Dynamical Systems for Observation-Consistent Inversion***](https://www.sciencedirect.com/science/article/pii/S0045782521005582), Steven Mattis, Kyle Robert Steffen, Troy Butler, Clint N. Dawson, Donald Estep, Computer Methods in Applied Mechanics and Engineering, Vol. 388, 114230, (2022)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### <center>Some references for the density-based approach\n",
    "---\n",
    "\n",
    "- [***Combining Push-Forward Measures and Bayes' Rule to Construct Consistent Solutions to Stochastic Inverse Problems***](https://epubs.siam.org/doi/abs/10.1137/16M1087229), T. Butler, J. Jakeman, T. Wildey, SIAM J. Sci. Comput., 40(2), A984–A1011 (2018)\n",
    "\n",
    "\n",
    "- [***Convergence of Probability Densities using Approximate Models for Forward and Inverse Problems in Uncertainty Quantification***](https://epubs.siam.org/doi/abs/10.1137/18M1181675), T. Butler, J. Jakeman, T. Wildey, SIAM J. Sci. Comput., 40(5), A3523–A3548. (2018)\n",
    "\n",
    "\n",
    "- [***Data-consistent inversion for stochastic input-to-output maps***](https://iopscience.iop.org/article/10.1088/1361-6420/ab8f83), T. Butler, T. Wildey, T.Y. Yen, Inverse Problems, 36, 085015, (2020)\n",
    "\n",
    "\n",
    "- [***Convergence of Probability Densities using Approximate Models for Forward and Inverse Problems in Uncertainty Quantification: Extensions to $L^p$***](http://arxiv.org/abs/2001.04369), T. Butler, T. Wildey, W. Zhang, International Journal for Uncertainty Quantification, in-press (2022)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## <center> Why use Jupyter notebooks? It keeps me honest.\n",
    "___\n",
    "\n",
    "<center><img src=\"figures/dt160106.gif\" width=70%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### <center> There is more to this notebook than what is shown in the presentation.\n",
    "    \n",
    "You are invited to explore this and other notebooks at https://github.com/CU-Denver-UQ/LUQ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### <center> Some perspective (thanks Mark Twain!)\n",
    "___\n",
    "    \n",
    "\n",
    "* Facts are stubborn, but statistics are more pliable.\n",
    "\n",
    "\n",
    "* Most people use statistics like a drunk man uses a lamppost; more for support than illumination.\n",
    "\n",
    "\n",
    "* <font color='blue'>***Data is like garbage. You'd better know what you are going to do with it before you collect it.***</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### <center> Building Intuition\n",
    "___\n",
    "    \n",
    "<center><img src=\"figures/inf_pred_cartoon.png\" width=70%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### <center> A Motivating Example: \"Cooking\" a T-Bone\n",
    "___\n",
    "\n",
    "Step 1: Build a 3D model (5341 DOF, 23498 tetrahedral elements).\n",
    "\n",
    "<center><video controls src=\"T-bone-mesh.mp4\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Step 2: Determine thermal transfer properties of meat, bone, metal skillets, heated air, and typical temperature settings of stove tops.\n",
    "\n",
    "\n",
    "Step 3: Follow (i.e., code) directions given by Omaha Steaks!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### <center> A Motivating Example: \"Cooking\" a T-Bone\n",
    "___\n",
    "\n",
    "Representative steak simulated: Rare (blue), Medium rare (red), Medium-to-well done (brown)\n",
    "\n",
    "<center><video controls src=\"T-bone-medium-rare.mp4\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### <center> The Snowflake (not millenials) Analogy \n",
    "___\n",
    "    \n",
    "\n",
    "* Thermal properties are <font color='blue'>uncertain</font> due to natural variability in livestock. \n",
    "\n",
    "\n",
    "* <font color = 'blue'>Quantifying uncertainties</font> in the thermal properties enables us to cook steaks more reliably to their desired doneness. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "* Initial guesses of variability in thermal diffusivities of proten vs. bone will lead to some prediction of the distribution of final cooking temperatures.\n",
    "\n",
    "\n",
    "* Actual cooking temperatures from experiments are likely to follow a different distribution because initial guesses and predictions are typically wrong. \n",
    "\n",
    "\n",
    "* How do we reconcile these differences between what is observed and what is predicted to update initial distributions to be consistent with the data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center><img src=\"figures/t_bone_all_in_one.png\" width=70%/> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### <center> Notation and Terminology\n",
    "___\n",
    "\n",
    " * $\\lambda\\in\\Lambda$ = model inputs referred to as <font color='blue'>model parameters.</font>\n",
    " \n",
    " \n",
    " * $Q(\\lambda)$ = measurable model outputs referred to as <font color='blue'>quantities of interest (QoI).</font>\n",
    " \n",
    " \n",
    " * $\\mathcal{D} = Q(\\Lambda)$ denotes the set of observable data that can be <font color='red'>predicted by the model</font>.\n",
    " <br>\n",
    " \n",
    "     * $q$ is used to denote a single datum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### <center> Defining the problem and solution\n",
    "___\n",
    "Given a probability measure $P_\\mathcal{D}^{obs}$ on $\\mathcal{D}$ (described as density $\\pi_\\mathcal{D}^{obs}$) describing uncertainty in observed data, determine a probability measure $P_\\Lambda$ on $\\Lambda$ (described as density $\\pi_\\Lambda$) that is <font color='purple'>***consistent***</font> with both the model and observed data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "   \n",
    "Consistency means the <font color='blue'>***push-forward measure*** of $P_\\Lambda$ through the QoI map $Q(\\lambda)$ matches the probability measure given on $\\mathcal{D}$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "In other words, we want $P_\\Lambda$ to be a <font color='blue'>***pullback measure.***</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "To do this, we utilize the <font color='blue'>***push-forward of a initial $\\pi_\\Lambda^{initial}(\\lambda)$ on $\\Lambda$***</font>:\n",
    "\n",
    " \n",
    " * $\\pi_{\\mathcal{D}}^{predict}$ denotes the push-forward of the initial.\n",
    " \n",
    "     * Represents a initial ***prediction*** of the relative likelihood of model outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### <center> A pullback density: Existence, Uniqueness (up to initial), and Stability\n",
    "___\n",
    "$$\n",
    "  \\large \\pi^{update}_\\Lambda(\\lambda) = \\pi_\\Lambda^{initial}(\\lambda) \\frac{\\pi_{\\mathcal{D}}^{obs}(Q(\\lambda))}{\\pi_{\\mathcal{D}}^{predict}(Q(\\lambda))} \n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Solving a <font color='blue'>***forward UQ problem***</font> to approximate $\\pi_{\\mathcal{D}}^{predict}$ provides <font color='blue'>***sufficient***</font> information to <font color='blue'>***uniquely define***</font> $\\pi^{update}_\\Lambda$.\n",
    "\n",
    "\n",
    "* **<font color='purple'>Predictability Assumption</font>**: $\\exists$ $C>0$ such that $\\pi_{\\mathcal{D}}^{obs}(q) \\leq C\\pi_{\\mathcal{D}}^{predict}(q)$ for a.e. $q\\in\\mathcal{D}$.\n",
    "\n",
    "\n",
    "* $\\pi^{update}_\\Lambda$ is <font color='blue'>***stable***</font> (in the TV-metric) w.r.t. perturbations in $\\pi_\\Lambda^{initial}$ and $\\pi_{\\mathcal{D}}^{obs}$.\n",
    "\n",
    "\n",
    "* The error in the approximate $\\pi^{update}_\\Lambda$ is bounded by the error in the approximate $\\pi_{\\mathcal{D}}^{predict}$ , i.e., the approximations are <font color='blue'>***numerically stable.***</font> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### <center> Role of the set-valued inverses\n",
    "___\n",
    "$$\n",
    "  \\large \\pi^{update}_\\Lambda(\\lambda) = \\pi_\\Lambda^{initial}(\\lambda) \\frac{\\pi_{\\mathcal{D}}^{obs}(Q(\\lambda))}{\\pi_{\\mathcal{D}}^{predict}(Q(\\lambda))} \n",
    "$$\n",
    "    \n",
    "* $\\pi_{\\mathcal{D}}^{predict}(Q(\\lambda))$ and $\\pi_{\\mathcal{D}}^{obs}(Q(\\lambda))$ are <font color='blue'>***constant*** on each set-valued inverse $Q^{-1}(q)$.</font>\n",
    "\n",
    "\n",
    "* $\\pi_\\Lambda^{initial}(\\lambda)$ serves to regularize the pullback only <font color='blue'>***along*** on each set-valued inverse $Q^{-1}(q)$.</font>\n",
    "\n",
    "\n",
    "* <font color='purple'>$\\pi^{update}_\\Lambda(\\lambda)$ is an update to $\\pi_\\Lambda^{initial}(\\lambda)$ only in directions locally orthogonal to set-valued inverses. </font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### <center> A simple example in Python: The Spaces, QoI, and Densities\n",
    "___\n",
    "\n",
    "\n",
    "* $\\Lambda=[-1,1]$.\n",
    "\n",
    "* $Q(\\lambda) = \\lambda^p$ for $p=5$.\n",
    "\n",
    "* $\\mathcal{D} = Q(\\Lambda) = [-1,1]$.\n",
    "\n",
    "___\n",
    "\n",
    "* $\\pi_{\\Lambda}^{initial} \\sim U([-1,1])$\n",
    "\n",
    "* $\\pi_{\\mathcal{D}}^{obs} \\sim N(\\mu,\\sigma^2)$\n",
    "    * Initially take $\\mu=0.25$ and $\\sigma=0.1$.\n",
    "\n",
    "* $\\pi_{\\mathcal{D}}^{predict}$ \n",
    "    * Known in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def QoI(lam,p): # defing a QoI mapping function\n",
    "    q = lam**p\n",
    "    return q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "N, mu, sigma = int(1E4), 0.25, 0.1 # number of samples from initial and observed mean (mu) and st. dev (sigma)\n",
    "lam = np.random.uniform(low=-1,high=1,size=N) # sample set of the initial\n",
    "\n",
    "# Evaluate the QoI map on this initial sample set\n",
    "qvals_nonlinear = QoI(lam,5) # Evaluate lam^5 samples\n",
    "\n",
    "# Estimate the push-forward density for the QoI\n",
    "predict_kde = kde( qvals_nonlinear )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12,6)) \n",
    "# Plot the push-forward of the initial and observed densities\n",
    "\n",
    "qplot = np.linspace(-1,1, num=100)\n",
    "observed_plot = plt.plot(qplot,norm.pdf(qplot, loc=mu, scale=sigma), 'r-', linewidth=4, label=\"$\\pi_\\mathcal{D}^{obs}$\")\n",
    "\n",
    "# Plot the exact predicted density\n",
    "pf_initial_plot = plt.plot(qplot,1/10*np.abs(qplot)**(-4/5),'b-', linewidth=4, label=\"$\\pi_\\mathcal{D}^{predict}$\")\n",
    "\n",
    "plt.xlim([-1,1])\n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "plt.xlabel(\"$\\mathcal{D}$\", fontsize=24)\n",
    "plt.legend(fontsize=20)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12,6))  \n",
    "# Plot the initial and update\n",
    "\n",
    "lam_plot = np.linspace(-1,1, num=100)\n",
    "initial_plot = plt.plot(lam_plot,uniform.pdf(qplot, loc=-1, scale=2), 'b-', linewidth=4, label=\"$\\pi_\\Lambda^{initial}$\")\n",
    "pullback_plot = plt.plot(lam_plot,uniform.pdf(qplot, loc=-1, scale=2)*\\\n",
    "                         norm.pdf(QoI(lam_plot,5), loc=mu, scale=sigma)/(1/10*np.abs(QoI(lam_plot,5))**(-4/5)),'k-', linewidth=4, label=\"$\\pi_\\Lambda$\")\n",
    "plt.xlim([-1,1])\n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "plt.xlabel(\"$\\Lambda$\",fontsize=24)\n",
    "plt.legend(fontsize=20)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### <center> A simple example in Python: Approximations and Sampling $\\pi^{update}_\\Lambda$\n",
    "___\n",
    "\n",
    "\n",
    "* **<font color='purple'>Goal:</font>** Use initial samples to generate samples from $\\pi^{update}_\\Lambda$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "#### Investigate  [rejection sampling](https://en.wikipedia.org/wiki/Rejection_sampling):\n",
    "___\n",
    "**<font color='purple'>Goal:</font>** Use samples from a random variable with density $g$ to generate samples from a random variable with density $f$. \n",
    "\n",
    "\n",
    "**<font color='purple'>Requirement:</font>** There exists $M>0$ such that $f(x)\\leq M g(x)$ for all $x$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "**<font color='purple'>Algorithm:</font>**\n",
    "For proposed sample $y$ from $g$ \n",
    "* Generate sample $u \\sim \\mathrm {Unif} (0,1)$.\n",
    "* If $u<f(y)/Mg(y)$, <font color='blue'>accept</font> $y$ as sample from $f$, otherwise reject."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### <center> A simple example in Python: Sampling $\\pi^{update}_\\Lambda$\n",
    "___\n",
    "**<font color='purple'>Rejection sampling for $\\pi^{update}_\\Lambda$?</font>**\n",
    "    \n",
    "$g=\\pi_\\Lambda^{initial}$ and $f=\\pi^{update}_\\Lambda$, so can use rejection sampling if there exists $M>0$ s.t.\n",
    "$$\n",
    "    \\pi_\\Lambda^{initial}(\\lambda) \\frac{\\pi_{\\mathcal{D}}^{obs}(Q(\\lambda))}{\\pi_{\\mathcal{D}}^{predict}(Q(\\lambda))} \\leq M \\pi_\\Lambda^{initial}(\\lambda).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "**<font color='purple'>Some manipulations and an observation</font>**\n",
    "\n",
    "Divide through by the $\\pi_\\Lambda^{initial}$ and multiply by $\\pi_{\\mathcal{D}}^{predict}$ to both sides to see\n",
    "$$\n",
    "    \\pi_{\\mathcal{D}}^{obs}(Q(\\lambda)) \\leq M \\pi_{\\mathcal{D}}^{predict}(Q(\\lambda)).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<font color='purple'>***The predictability assumption guarantees the existence of $M>0$.***</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### <center> A simple example in Python: Something useful\n",
    "___\n",
    "    \n",
    "$$\n",
    "    \\text{We rewrite } \\pi^{update}_\\Lambda(\\lambda) = \\pi_\\Lambda^{initial}(\\lambda)r(Q(\\lambda)), \\text{ where } r(Q(\\lambda)) := \\frac{\\pi_{\\mathcal{D}}^{obs}(Q(\\lambda))}{\\pi_{\\mathcal{D}}^{predict}(Q(\\lambda))}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "* $r(Q(\\lambda))$ defines a re-weighting of the initial and its samples.\n",
    "\n",
    "* Can estimate $M>0$ by maximum of $r(Q(\\lambda))$ on the initial sample set.\n",
    "\n",
    "* Rejection sampling done by comparing $r(Q(\\lambda))/M$ evaluated on initial sample set to uniform random variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def rejection_sampling(r):\n",
    "    N = r.size  # size of proposal sample set\n",
    "    check = np.random.uniform(low=0,high=1,size=N)  # create random uniform weights to check r against\n",
    "    M = np.max(r)\n",
    "    new_r = r/M  # normalize weights \n",
    "    idx = np.where(new_r>=check)[0]  # rejection criterion\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### <center> A simple example in Python: Sampling time!\n",
    "___\n",
    "\n",
    "We now compute $r(Q(\\lambda))$, perform rejection sampling, and push-forward these samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Evaluate the observed density on the QoI sample set\n",
    "obs_vals_nonlinear = norm.pdf(qvals_nonlinear, loc=mu, scale=sigma)\n",
    "\n",
    "# Compute r\n",
    "r_nonlinear = np.divide(obs_vals_nonlinear,1/10*np.abs(qvals_nonlinear)**(-4/5))\n",
    "\n",
    "# Perform rejection sampling\n",
    "samples_to_keep_nonlinear = rejection_sampling(r_nonlinear)\n",
    "update_lam_nonlinear = lam[samples_to_keep_nonlinear]\n",
    "\n",
    "# Construct push-forward of the pullback density\n",
    "update_q_nonlinear = qvals_nonlinear[samples_to_keep_nonlinear] \n",
    "pf_update_kde = kde( update_q_nonlinear ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,6)) \n",
    "\n",
    "# Plot the push-forward of the initial, observed density, and push-forward of pullback\n",
    "\n",
    "qplot = np.linspace(-1,1, num=100)\n",
    "observed_plot = plt.plot(qplot,norm.pdf(qplot, loc=mu, scale=sigma), 'r-', linewidth=4, label=\"$\\pi_\\mathcal{D}^{obs}$\")\n",
    "pf_initial_plot = plt.plot(qplot,1/10*np.abs(qplot)**(-4/5),'b-', linewidth=4, label=\"$\\pi_\\mathcal{D}^{predict}$\")\n",
    "pf_post_plot = plt.plot(qplot, pf_update_kde(qplot),'k--', linewidth=4, label=\"PF of $\\pi_\\Lambda$\")\n",
    "\n",
    "plt.xlim([-1,1]), plt.xlabel(\"$\\mathcal{D}$\", fontsize=20)\n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "plt.legend(fontsize=20)\n",
    "title_str = 'Accepted %4.2f' %(update_q_nonlinear.size/N*100) +'% of ' + str(N) + ' initial samples'\n",
    "plt.title(title_str)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### <center> A simple example in Python: Evaluating the solution\n",
    "___\n",
    "    \n",
    "#### Quality of solution? \n",
    "\n",
    "* Statistics on $\\pi^{update}_\\Lambda$ are easily estimated using the accepted sample set, but it is <font color='red'>***unusual to know what the mean or standard deviation of $\\pi^{update}_\\Lambda$ should be***</font>.\n",
    "\n",
    "* <font color='purple'>***We know what statistics <font color='red'>should be</font> on the push-forward of $\\pi^{update}_\\Lambda$.***</font>\n",
    "  \n",
    "    * Check by computing sample statistics on accepted samples.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "printmd('<font size=6pt>**$\\mathbb{E}$(pf of $\\pi^{update}_\\Lambda$) = %4.3f**' %np.mean(update_q_nonlinear) + '  <font size=6pt>**vs. $\\mathbb{E}(\\pi_\\mathcal{D}^{obs}$) = %4.3f**' %mu) # Should be about mu\n",
    "printmd('<font size=6pt>**St. dev. of pf of $\\pi^{update}_\\Lambda$ = %4.3f**' %np.std(update_q_nonlinear) + '  <font size=6pt>**vs. St. dev. of $\\pi_\\mathcal{D}^{obs}$ = %4.3f**' %sigma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### <center> A simple example in Python: Evaluating the solution\n",
    "___\n",
    "    \n",
    "Statistics only tell part of the story. We should also check two things:\n",
    "\n",
    " - Did we satisfy the predictability assumption?\n",
    " \n",
    " \n",
    " - What did we gain by solving the inverse problem? (Use KL divergence, skipping in this talk but it is in the notebook)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### <center> A simple example in Python: Developing a diagnostic\n",
    "___\n",
    "#### Did we satisfy the predictability assumption?\n",
    "\n",
    "Predictability assumption satisfied implies $\\int_\\Lambda \\pi^{update}\\Lambda(\\lambda)\\, d\\mu_\\Lambda=1$, so\n",
    "\n",
    "$$\n",
    "     1 = \\int_\\Lambda \\pi_\\Lambda^{initial}(\\lambda)r(Q(\\lambda))\\, d\\mu_\\Lambda = \\int_\\Lambda r(Q(\\lambda))\\, dP_\\Lambda^{initial} = \\mathbb{E}(r(Q(\\lambda)))\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "___\n",
    "**<font color='purple'>Diagnostic for Verifying Predictability Assumption:</font>** ***Sample mean of $r(Q(\\lambda))$ is approximately 1.***\n",
    "\n",
    "Small deviations from 1 are usually expected and indicate finite sampling errors or errors in approximating $\\pi_\\mathcal{D}^{predict}$. (There are exceptions.) What was our sample mean?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "printmd('<font size=8pt color=blue>**$\\mathbb{E}(r) =$ %10.2f**' %(np.mean(r_nonlinear)) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### <center> A simple example in Python: Approximations and the diagnostic\n",
    "___\n",
    "\n",
    "* In general, we approximate $\\pi_{\\mathcal{D}}^{predict}$ with finite sampling and KDE (which introduces errors).\n",
    "\n",
    "    * Let $\\pi_{\\mathcal{D},N}^{predict}$ and $\\pi^{update}_{\\Lambda,N}$ denote the resulting approximate PDFs.\n",
    "\n",
    "\n",
    "* **<font color='purple'>Goal:</font>** Re-use samples generating $\\pi_{\\mathcal{D},N}^{predict}$ to generate samples from $\\pi^{update}_\\Lambda$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure() \n",
    "\n",
    "# Plot the push-forward of the initial and observed densities\n",
    "\n",
    "qplot = np.linspace(-1,1, num=100)\n",
    "observed_plot = plt.plot(qplot,norm.pdf(qplot, loc=mu, scale=sigma), 'r-', linewidth=4, label=\"$\\pi_\\mathcal{D}^{obs}$\")\n",
    "pf_initial_plot = plt.plot(qplot,1/10*np.abs(qplot)**(-4/5),'b-', linewidth=4, label=\"$\\pi_\\mathcal{D}^{predict}$\")\n",
    "pf_initial_plot = plt.plot(qplot,predict_kde(qplot),'b--', linewidth=4, label=\"$\\pi_{\\mathcal{D},N}^{predict}$\")\n",
    "\n",
    "plt.xlim([-1,1])\n",
    "plt.xlabel(\"$\\mathcal{D}$\", fontsize=24)\n",
    "plt.legend(fontsize=20)\n",
    "plt.title('Sample Size = ' + str(N))\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Compute approximate r using approx pf and compute its mean\n",
    "r_nonlinear_approx = np.divide(obs_vals_nonlinear,predict_kde(qvals_nonlinear))\n",
    "#print(np.mean(r_nonlinear_approx))\n",
    "printmd('<font size=6pt>**Using $\\pi_{\\mathcal{D},N}^{predict}$, $\\mathbb{E}(r) =$ %10.2f**' %(np.mean(r_nonlinear_approx)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Perform rejection sampling\n",
    "samples_to_keep_nonlinear = rejection_sampling(r_nonlinear_approx)\n",
    "post_lam_nonlinear = lam[samples_to_keep_nonlinear]\n",
    "\n",
    "# Construct push-forward of the pullback density\n",
    "update_q_nonlinear = qvals_nonlinear[samples_to_keep_nonlinear] \n",
    "pf_update_kde = kde( update_q_nonlinear ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,6)) \n",
    "# Plot the push-forward of the initial, observed density, and push-forward of pullback\n",
    "\n",
    "qplot = np.linspace(-1,1, num=100)\n",
    "\n",
    "observed_plot = plt.plot(qplot,norm.pdf(qplot, loc=mu, scale=sigma), 'r-', linewidth=4, label=\"$\\pi_\\mathcal{D}^{obs}$\")\n",
    "approx_pf_initial_plot = plt.plot(qplot,predict_kde(qplot),'b--', linewidth=4, label=\"${\\pi}_{\\mathcal{D},N}^{predict}$\")\n",
    "pf_initial_plot = plt.plot(qplot,1/10*np.abs(qplot)**(-4/5),'b-', linewidth=4, label=\"$\\pi_\\mathcal{D}^{predict}$\")\n",
    "pf_post_plot = plt.plot(qplot,pf_update_kde(qplot),'k--', linewidth=4, label=\"PF of $\\pi^{update}_{\\Lambda,N}$\")\n",
    "\n",
    "title_str = 'Using $\\pi_{\\mathcal{D},N}^{predict}$, $\\mathbb{E}(r) =$%2.3f' %(np.mean(r_nonlinear_approx))\n",
    "plt.title(title_str, fontsize=20)\n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "plt.xlim([-1,1]), plt.xlabel(\"$\\mathcal{D}$\", fontsize=24), plt.legend(fontsize=20), plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### <center> A simple example in Python: Information gain\n",
    "___\n",
    "\n",
    "#### What did we gain by solving the inverse problem?\n",
    "\n",
    "<font color='blue'>***Information gain***</font> quantified by the [Kullback-Leibler divergence](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence) (KL divergence)\n",
    "    \n",
    "$$\n",
    "\\begin{eqnarray*}\n",
    "    KL(\\pi_{\\Lambda}^{update}\\, : \\, \\pi_{\\Lambda}^{initial}) &=& \\int_\\Lambda \\pi_\\Lambda^{update}(\\lambda)\\log\\left(\\frac{\\pi_\\Lambda^{update}(\\lambda)}{\\pi_\\Lambda^{initial}(\\lambda)}\\right)\\, d\\mu_\\Lambda \\\\ &=& \\int_\\Lambda r(Q(\\lambda))\\log r(Q(\\lambda))\\, dP_\\Lambda^{initial}.\n",
    "\\end{eqnarray*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "**<font color='purple'> Understanding the KL divergence:</font>**\n",
    "$KL(\\pi_{\\Lambda}^{update} \\, : \\, \\pi_{\\Lambda}^{initial}) \\geq 0$.\n",
    "* Larger values indicate the posterior differs more significantly from the initial. \n",
    "        \n",
    "* Different initials or different QoI maps clearly can impact the KL divergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "print(np.mean(r_nonlinear*np.log(r_nonlinear)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### <center> A simple example in Python: Solving multiple inverse problems\n",
    "___\n",
    "\n",
    "#### Investigating different observed densities\n",
    "\n",
    "Once  $\\pi_\\mathcal{D}^{predict}$ is constructed, $\\pi^{update}_\\Lambda$ is trivial to define for any observed density <font color='purple'>***as long as the predictability assumption holds.***</font> \n",
    "\n",
    "By **<font color='blue'>trivial</font>**, we mean that formulating $\\pi^{update}_\\Lambda$ does <font color='purple'>***not require any additional model solves.***</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# New observed density centered at 0\n",
    "obs_vals_nonlinear_new = norm.pdf(qvals_nonlinear, loc=-0.6, scale=0.1) \n",
    "\n",
    "# Compute r\n",
    "r_nonlinear_new = np.divide(obs_vals_nonlinear_new,predict_kde(qvals_nonlinear))\n",
    "print(np.mean(r_nonlinear_new))\n",
    "\n",
    "# Rejection sampling\n",
    "samples_to_keep_nonlinear_new = rejection_sampling(r_nonlinear_new)\n",
    "update_lam_nonlinear_new = lam[samples_to_keep_nonlinear_new]\n",
    "\n",
    "# Approximate the push-forward of pullback\n",
    "update_q_nonlinear_new = qvals_nonlinear[samples_to_keep_nonlinear_new]\n",
    "pf_update_new_kde = kde( update_q_nonlinear_new )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,6)) \n",
    "\n",
    "qplot = np.linspace(-1,1, num=100)\n",
    "\n",
    "observed_plot = plt.plot(qplot,norm.pdf(qplot, loc=-0.6, scale=0.1), 'r-', linewidth=4, label=\"$\\pi_\\mathcal{D}^{obs}$\")\n",
    "pf_initial_plot = plt.plot(qplot,1/10*np.abs(qplot)**(-4/5),'b-', linewidth=4, label=\"$\\pi_\\mathcal{D}^{predict}$\")\n",
    "pf_initial_plot = plt.plot(qplot,predict_kde(qplot),'b--', linewidth=4, label=\"$\\pi_{\\mathcal{D},N}^{predict}$\")\n",
    "pf_post_plot = plt.plot(qplot,pf_update_new_kde(qplot),'k--', linewidth=4, label=\"PF of $\\pi^{update}_{\\Lambda,N}$\")\n",
    "\n",
    "title_str = 'Using $\\pi_{\\mathcal{D},N}^{predict}$, $\\mathbb{E}(r) =$%2.3f' %(np.mean(r_nonlinear_new))\n",
    "plt.title(title_str, fontsize=20)\n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "plt.xlim([-1,1]), plt.xlabel(\"$\\mathcal{D}$\", fontsize=24), plt.legend(fontsize=20), plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# New observed density centered at 0\n",
    "obs_vals_nonlinear_new = norm.pdf(qvals_nonlinear, loc=1, scale=0.1) \n",
    "\n",
    "# Compute r\n",
    "r_nonlinear_new = np.divide(obs_vals_nonlinear_new,predict_kde(qvals_nonlinear))\n",
    "print(np.mean(r_nonlinear_new))\n",
    "\n",
    "# Rejection sampling\n",
    "samples_to_keep_nonlinear_new = rejection_sampling(r_nonlinear_new)\n",
    "update_lam_nonlinear_new = lam[samples_to_keep_nonlinear_new]\n",
    "\n",
    "# Approximate the push-forward of pullback\n",
    "update_q_nonlinear_new = qvals_nonlinear[samples_to_keep_nonlinear_new]\n",
    "pf_update_new_kde = kde( update_q_nonlinear_new )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,6)) \n",
    "\n",
    "qplot = np.linspace(-1,1, num=100)\n",
    "\n",
    "observed_plot = plt.plot(qplot,norm.pdf(qplot, loc=1, scale=0.1), 'r-', linewidth=4, label=\"$\\pi_\\mathcal{D}^{obs}$\")\n",
    "pf_initial_plot = plt.plot(qplot,1/10*np.abs(qplot)**(-4/5),'b-', linewidth=4, label=\"$\\pi_\\mathcal{D}^{predict}$\")\n",
    "pf_initial_plot = plt.plot(qplot,predict_kde(qplot),'b--', linewidth=4, label=\"$\\pi_{\\mathcal{D},N}^{predict}$\")\n",
    "pf_post_plot = plt.plot(qplot,pf_update_new_kde(qplot),'k--', linewidth=4, label=\"PF of $\\pi^{update}_{\\Lambda,N}$\")\n",
    "\n",
    "title_str = 'Using $\\pi_{\\mathcal{D},N}^{predict}$, $\\mathbb{E}(r) =$%2.3f' %(np.mean(r_nonlinear_new))\n",
    "plt.title(title_str, fontsize=20)\n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "plt.xlim([-1,1]), plt.xlabel(\"$\\mathcal{D}$\", fontsize=24), plt.legend(fontsize=20), plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### <center> A statistical Bayesian perspective\n",
    "___\n",
    "\n",
    "$$\n",
    "    {\\pi}^\\text{update}_\\Lambda(\\lambda \\, | \\, q) = \\pi_\\Lambda^{initial}\\frac{ L(q\\, |\\,  \\lambda) }{C}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    " * $L(q\\, | \\, \\lambda)$ is a ***data likelihood*** function that is not necessarily the same as the observed density.\n",
    "\n",
    "\n",
    " * $C$ is a normalizing constant.\n",
    " \n",
    "     * Usually not important when generating samples from the posterior using MCMC.\n",
    "     \n",
    "     * We compute it only so we can compare to our posterior by using rejection sampling.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### <center> A simple example in Python: Comparison to statistical Bayesian\n",
    "___\n",
    "\n",
    "We use the same initial and make the data likelihood function match the observed density."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# compute normalizing constants\n",
    "C_nonlinear = np.mean(obs_vals_nonlinear)\n",
    "sbayes_r_nonlinear = obs_vals_nonlinear/C_nonlinear\n",
    "\n",
    "# Perform rejection sampling\n",
    "sbayes_samples_to_keep_nonlinear = rejection_sampling(sbayes_r_nonlinear)\n",
    "sbayes_post_lam_nonlinear = lam[sbayes_samples_to_keep_nonlinear]\n",
    "\n",
    "# Construct push-forward of statistical Bayesian posterior\n",
    "sbayes_post_q_nonlinear = qvals_nonlinear[sbayes_samples_to_keep_nonlinear]\n",
    "sb_postq_nl_kde = kde( sbayes_post_q_nonlinear )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,6)) \n",
    "\n",
    "# Plot the push-forward of the initial, observed density, and push-forward of pullback and stats posterior\n",
    "qplot = np.linspace(-1,1, num=100)\n",
    "\n",
    "observed_plot = plt.plot(qplot,norm.pdf(qplot, loc=mu, scale=sigma), 'r-', linewidth=4, label=\"$\\pi_\\mathcal{D}^{obs}$\")\n",
    "pf_initial_plot = plt.plot(qplot,predict_kde(qplot),'b-', linewidth=4, label=\"$\\pi_{\\mathcal{D},N}^{predict}$\")\n",
    "pf_post_plot = plt.plot(qplot,pf_update_kde(qplot),'k--', linewidth=4, label=\"PF of $\\pi^{update}_{\\Lambda,N}$\")\n",
    "pf_sb_post_plot = plt.plot(qplot,sb_postq_nl_kde(qplot),'g--', linewidth=4, label=\"PF of posterior\")\n",
    "\n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "plt.xlim([-1,1]), plt.xlabel(\"$\\mathcal{D}$\", fontsize=24), plt.legend(fontsize=20), plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<a id='Dimension'></a>\n",
    "### <center> Discretized PDE Example\n",
    "___\n",
    "<font color='gray'>\n",
    "\\begin{equation*}\n",
    "\\begin{cases}\n",
    "-\\nabla \\cdot (K(\\lambda) \\nabla u) = 0, & (x,y)\\in\\Omega = (0,1)^2,\\\\\n",
    "u = 1, & x=0, \\\\\n",
    "u = 0, & x=1, \\\\\n",
    "K(\\lambda)\\nabla p \\cdot \\mathbf{n} = 0, & y=0 \\text{ and } y=1.\n",
    "\\end{cases}\n",
    "\\end{equation*}\n",
    "    </font>\n",
    "\n",
    "<br>\n",
    "\n",
    "<center><font color='gray'>Diffusion field defined by $Y = \\log{K}$ and $Y(\\lambda) = \\overline{Y} + \\sum_{i=1}^\\infty \\xi_i(\\lambda)\\sqrt{\\eta_i}f_i(x,y)$.</font> </center>\n",
    "\n",
    "<br>\n",
    "\n",
    "<center> <font color='blue'>Truncate KL expansion at 100 terms and use $\\pi_\\Lambda^{initial}\\sim N(0,I)$.</font> </center>\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### <center> Are we really going to perform accept/reject in 100D?\n",
    "___\n",
    "    \n",
    "No! The computations are really taking place in the 1D data space!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "data_set = sio.loadmat('elliptic_kde100_10K.mat')\n",
    "qvals = data_set['qq'] # QoI samples\n",
    "qvals = qvals[:,0] # Only using first QoI here\n",
    "lam_100D = data_set['pp'] # parameter samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "obs_vals = norm.pdf(qvals, loc=0.7, scale=1.0e-2) # Define an observed density\n",
    "predict_kde = kde( qvals, 'silverman' )\n",
    "\n",
    "r = np.divide(obs_vals,predict_kde(qvals))\n",
    "print(np.mean(r))\n",
    "\n",
    "samples_to_keep = rejection_sampling(r)\n",
    "update_lam_100D = lam_100D[:,samples_to_keep]\n",
    "update_q = qvals[samples_to_keep]\n",
    "pf_update_kde = kde(update_q)\n",
    "accept_rate = samples_to_keep.size/lam.shape[0]\n",
    "print(accept_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,6)) # Plot data space densities\n",
    "qplot = np.linspace(0.6, 0.9, num=100) \n",
    "\n",
    "oplot = plt.plot(qplot,norm.pdf(qplot, loc=0.7, scale=1.0e-2), 'r-', linewidth=4, label=\"$\\pi_\\mathcal{D}^{obs}$\")\n",
    "prplot = plt.plot(qplot,predict_kde(qplot),'b-', linewidth=4, label=\"$\\pi_{\\mathcal{D},N}^{predict}$\")\n",
    "poplot = plt.plot(qplot,pf_update_kde(qplot),'k--', linewidth=4, label=\"PF of $\\pi^{update}_{\\Lambda,N}$\")\n",
    "\n",
    "title_str = '$\\mathbb{E}(r) =$%2.3f' %(np.mean(r))\n",
    "plt.title(title_str, fontsize=20)\n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "plt.xlim([0.6,0.9]), plt.xlabel(\"$\\mathcal{D}$\", fontsize=24), plt.legend(fontsize=20), plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "# Visualize updated statistics in 100-D\n",
    "\n",
    "param = np.arange(100)\n",
    "plt.xticks(fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.barh(param,np.mean(update_lam_100D,1)), plt.xlim([-0.4,0.4]), plt.xlabel(\"Mean of marginals of updated\", fontsize=24)\n",
    "plt.ylabel(\"KL Mode\", fontsize=24), plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### <center> How well do we scale with dimension?\n",
    "___\n",
    "    \n",
    "\n",
    "It all depends on which space $\\Lambda$ or $\\mathcal{D}$ has the **large** dimension."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### <center> Convergence and Dimension: An Example\n",
    "___\n",
    "   \n",
    "***Setup:***\n",
    "\n",
    "$\n",
    "\tQ(\\lambda) = (\\lambda-\\mu)^\\top C^{-1}(\\lambda-\\mu),\n",
    "$\n",
    "\n",
    "$\n",
    "\t\\pi_{\\mathcal{D}}^\\text{obs}\\sim U([a,b])^*\n",
    "$\n",
    "\n",
    "<br>\n",
    "\n",
    "***Prior and Push-forward:***\n",
    "\n",
    "$\n",
    "\t\\boxed{\\Lambda = \\mathbb{R}^{{d}}} \\ \\text{ and } \\  \\pi_\\Lambda^{initial} \\sim N(\\mu, C) \\ \\Rightarrow \\ \\pi_{\\mathcal{D}}^{predict} \\sim \\chi^2(d)\n",
    "$\n",
    "--\n",
    "\n",
    "---\n",
    "$^*$ $a$ and $b$ chosen as the $40$th and $60$th percentiles of $\\pi_{\\mathcal{D}}^{predict}$ for each $d$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### <center> Approx. error in $L^1$ of updated density vs. Num. Samples used in GKDE.\n",
    "___\n",
    "    \n",
    "  \n",
    "<center><img src=\"figures/convergence_linearQ_paramDim_d_vs_samples.png\" width=70%/> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### <center> Convergence and Dimension: An Example\n",
    "___\n",
    "   \n",
    "***Setup:***\n",
    "\n",
    "$\n",
    "    \\boxed{Q\\in\\mathbb{R}^{{m}}}, \tQ_i(\\lambda) = (\\lambda-\\mu_i)^\\top C_i^{-1}(\\lambda_i-\\mu),\n",
    "$\n",
    "--\n",
    "$\n",
    "\t\\pi_{\\mathcal{D}}^\\text{obs}\\sim U([a,b])^*\n",
    "$\n",
    "\n",
    "<br>\n",
    "\n",
    "***Prior and Push-forward:***\n",
    "\n",
    "$\n",
    "\t\\Lambda = \\mathbb{R}^{10}, \\pi_\\Lambda^{initial} \\sim N(\\mu, C) \\Rightarrow \\pi_{\\mathcal{D}_i}^{predict} \\sim \\chi^2 (10/m)\n",
    "$\n",
    "\n",
    "---\n",
    "$^*$ $a$ and $b$ chosen so Lebesgue measure of support of $\\pi_{\\mathcal{D}}^{obs}$ is constant for each $m$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### <center> Approx. error in $L^1$ of updated density vs. Num. Samples used in GKDE.\n",
    "___\n",
    "    \n",
    "<center><img src=\"figures/convergence_linearQ_QDim_m_vs_samples.png\" width=70%/> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### <center> So how do we address our curse of dimensionality?\n",
    "---\n",
    "    \n",
    "Really the topic for a completely different talk and involves on-going research on density-free approaches to data-consistent inversion. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### <center> Summary up to this point\n",
    "---\n",
    "   \n",
    "* The data-consistent solution exists, is unique (up to choice of  $\\pi_\\Lambda^{initial}(\\lambda)$), and is stable.\n",
    "\n",
    "   \n",
    "* The data-consistent solution is an update to $\\pi_\\Lambda^{initial}(\\lambda)$ only in directions locally orthogonal to set-valued inverses.\n",
    "\n",
    "\n",
    "* All computations take place in the (generally) lower-dimensional QoI space.\n",
    "\n",
    "    * Rejection sampling is often possible even if parameter spaces are high-dimensional.\n",
    "    \n",
    "\n",
    "* The ratio $r(Q(\\lambda))$ provides a useful diagnostic for verifying predictability assumption, performing rejection sampling, and is the critical component distinguishing this approach from classical Bayesian formulations of inverse problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### <center> From data to QoI\n",
    "___\n",
    "\n",
    "    \n",
    "<font color='blue'><center>***Data is like garbage. You'd better know what you are going to do with it before you collect it.*** - *Mark Twain*</center></font>\n",
    "<br>\n",
    "    \n",
    "- We have so far assumed that observable data is for some a priori specified QoI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- What if we collect data but we do <font color='red'>**not**</font> know the QoI?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Data is *not* the same thing as information and by itself is *not* interesting. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- In the era of *Big Data*, we often collect a lot of garbage and then go dumpster diving. Our apologies to Mark Twain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### <center> From data to QoI: Illustrative example\n",
    "___\n",
    "\n",
    "- $\\Lambda = [-1,1]^2\\subset\\mathbb{R}^2$\n",
    "\n",
    "- Dynamic response induced by parameters given by polynomial of unknown order.\n",
    "    \n",
    "- Observed data obtained at 10 Hz $[0,2]$ ($t$ assumed in seconds)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def poly(t, lam):\n",
    "    poly_order = np.random.choice([2, 3, 4, 5, 6], 1)[0]  # Randomly choose a 2nd to 6th order poly\n",
    "    print(poly_order)\n",
    "    num_params = lam.shape[0]\n",
    "    num_t = len(t)\n",
    "    p_vals = np.zeros( (num_params, num_t) )\n",
    "    for j in range(poly_order-1):\n",
    "        p_vals[:, :] += (2*np.random.uniform()-1)* t**j\n",
    "    for i in range(num_params):\n",
    "        p_vals[i, :] += lam[i,0] * t**(poly_order-1) + lam[i,1] * t**(poly_order)\n",
    "    return p_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "N = int(1e3)\n",
    "lam = np.random.uniform(low=-1, high=1, size=(N,2))\n",
    "t = np.linspace(0, 2, 21)\n",
    "data = poly(t[1:], lam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "data_scaled = StandardScaler().fit_transform(data) # scaling the samples\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.plot(t[1:], data_scaled.T[:,np.random.choice(N,50)]);\n",
    "plt.xlabel('time, $t$', fontsize=14), plt.ylabel('Responses', fontsize=14)\n",
    "plt.xticks(t[1:]), plt.grid(True, linestyle=':', lw='0.5')\n",
    "plt.title('50 (Random and Scaled) Responses', fontsize=16);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "pca = PCA(n_components=20) # initializing PCA\n",
    "principal_components = pca.fit_transform(data_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### <center> From data to QoI: Illustrative example\n",
    "___\n",
    "\n",
    "- Use PCA to <font color='blue'>***learn***</font> the number of interesting features in the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "%reset -f out\n",
    "\n",
    "def plot_gap(eig_vals, n):\n",
    "    fig = plt.figure(figsize=(8,4))\n",
    "    fig.clear()\n",
    "    plt.semilogy(np.arange(1,1+np.size(eig_vals)),eig_vals, marker='o', markersize=12, linestyle='')\n",
    "    plt.semilogy(np.arange(0,2+np.size(eig_vals)),eig_vals[n]*np.ones(np.size(eig_vals)+2), 'r--')\n",
    "    plt.semilogy(np.arange(0,2+np.size(eig_vals)),eig_vals[n+1]*np.ones(np.size(eig_vals)+2), 'r--')\n",
    "    s = 'Orders of magnitude between %d and %d e.vals is %4.2f' %(n+1, n+2, \n",
    "                                                                  np.log10(eig_vals[n])-np.log10(eig_vals[n+1]))\n",
    "    s += '\\n and e.vals up to %d account for %4.2f percent of variation' %(n+1, \n",
    "                                                                           np.sum(eig_vals[0:n+1])/np.sum(eig_vals)*100)\n",
    "    plt.title(s)\n",
    "    plt.xticks(np.arange(1, len(eig_vals)+1))\n",
    "    plt.grid(True, linestyle=':', lw=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "interact(plot_gap,\n",
    "         eig_vals = widgets.fixed(pca.singular_values_),\n",
    "         n = widgets.IntSlider(value=1, min=0, max=5));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### <center> From data to QoI: Illustrative example\n",
    "___\n",
    "\n",
    "- Construct the QoI map $Q(\\lambda)$ from what we learned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "## principal components plot ##\n",
    "plt.figure(figsize=(8,4)), plt.subplot(1,2,1), plt.scatter(lam[:,0], lam[:,1], c = principal_components[:,0])\n",
    "plt.xlabel(\"$\\lambda_1$\", fontsize=14), plt.ylabel(\"$\\lambda_2$\", fontsize=14)\n",
    "plt.title(\"Colored by Princ. Comp. 1\", fontsize=14)\n",
    "plt.subplot(1,2,2), plt.scatter(lam[:,0], lam[:,1], c=principal_components[:,1])\n",
    "plt.xlabel(\"$\\lambda_1$\", fontsize=14), plt.ylabel(\"$\\lambda_2$\", fontsize=14)\n",
    "plt.title(\"Colored by Princ. Comp. 2\", fontsize=14);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### <center> Then what? (and many additional \"what ifs\")\n",
    "___\n",
    "\n",
    "- Proceed as usual ***after*** transforming all time series of data (predicted and observed) into samples of QoI from which $\\pi_\\mathcal{D}^{predict}$ and $\\pi_\\mathcal{D}^{obs}$ can be estimated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- But, **what if** \n",
    "\n",
    "  - the responses had no closed form (i.e., there is *no* polynomial),\n",
    "  \n",
    "  - and **what if** all we have are noisy measurements with uncertain levels of noise, \n",
    "  \n",
    "  - and **what if** bifurcations occur in the dynamical response as parameters vary in ways that are not known a priori, \n",
    "  \n",
    "  - and **what if** the noisy observable data and computational data are available at potentially different frequencies, \n",
    "  \n",
    "  - and **what if** these data have differing levels of noise, \n",
    "  \n",
    "  - and **what if** there were potentially missing data values in a given time window?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Well, then we are in [`LUQ`](https://github.com/CU-Denver-UQ/LUQ)!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### <center> Learning Uncertain Quantities ([`LUQ`](https://github.com/CU-Denver-UQ/LUQ)) is as easy as 1-2-3\n",
    "___\n",
    "    \n",
    "- Step 1: Filter data (approximating dynamics) with adaptive piecewise linear splines\n",
    "<br>\n",
    "\n",
    "- Step 2: Clustering and classifying data (learning and classifying dynamics)\n",
    "<br>\n",
    "\n",
    "- Step 3: Feature extraction (learning quantities of interest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### <center> A typical LUQ implementation\n",
    "___\n",
    "    \n",
    "Step 0: Create a LUQ object (the hidden step)\n",
    "\n",
    "`from luq import LUQ  # Import LUQ module`<br>\n",
    "`learn = LUQ(predicted_time_series, observed_time_series, times)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Step 1: `learn.filter_data(**kwargs)`\n",
    "<br>\n",
    "\n",
    "- Step 2: `learn.dynamics(**kwargs)`\n",
    "<br>\n",
    "\n",
    "- Step 3: `predict_qoi, obs_qoi = learn.learn_qois_and_transform(**kwargs)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### <center> A simple example (harmonic oscillator): Step 1\n",
    "___\n",
    "\n",
    "<center>           |  <center>\n",
    ":-------------------------:|:-------------------------:\n",
    "<img src=\"figures/harmonic-oscillator-approximating-dynamics.png\" width=100%/>   |  <img src=\"figures/harmonic-oscillator-cleaning-data.png\" width=100%/> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### <center> A simple example (harmonic oscillator): Step 2\n",
    "___\n",
    "\n",
    "<center>\n",
    "<img src=\"figures/harmonic-oscillator-cluster-1.png\" width=100%/> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### <center> A simple example (harmonic oscillator): Step 3\n",
    "___\n",
    "\n",
    "<center>\n",
    "<img src=\"figures/harmonic-oscillator-PCs-cluster-1.png\" width=40%/>   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### <center> A simple example (harmonic oscillator): Updated densities on $c$ and $\\omega_0$\n",
    "___\n",
    "    \n",
    "\n",
    "<center>           |  <center>\n",
    ":-------------------------:|:-------------------------:\n",
    "<img src=\"figures/harmonic-oscillator-densities-c.png\" width=80%/>   |  <img src=\"figures/harmonic-oscillator-densities-omega.png\" width=80%/> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### <center> Not so simple examples (25 minutes just isn't enough time)\n",
    "___\n",
    "    \n",
    "    \n",
    "[***Learning Quantities of Interest from Dynamical Systems for Observation-Consistent Inversion***](https://www.sciencedirect.com/science/article/pii/S0045782521005582), has several other examples of interfacing [`LUQ`](https://github.com/CU-Denver-UQ/LUQ) with data-consistent inverse problems including \n",
    "\n",
    "- the Sel'kov model of glycolysis that exhibits a Hopf bifurcation as we vary kinetic parameters, \n",
    "\n",
    "- Burger's equation of a traveling wave exhibiting a shock forming at different speeds as we vary the initial condition, \n",
    "\n",
    "- the Advanced Circulation (ADCIRC) model for the shallow water equations modeling the variability of water elevations as a function of uncertain wind drag parameters during a simulated extreme weather event in the Shinnecock Inlet off the coast of Long Island, NY, USA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### <center> Ongoing and future LUQ and DCI work\n",
    "---\n",
    "    \n",
    "- Expanding LUQ to handle spatial and spatial-temporal data (utilizing deep-learning for the filtering step)\n",
    "  \n",
    "    \n",
    "- Statistical analyses of finite observational data impacts on DCI solutions (utilizing Dirichlet processes for analyzing uncertainties in observed density approximations)\n",
    "    \n",
    "\n",
    "- Empirical distribution methods for DCI solutions (utilizing constrained quadratic programs to construct optimal $L^2$ approximations)\n",
    "    \n",
    "\n",
    "- Parameter estimation within DCI using Maximal Updated Density (MUD) points (utilizing analysis of residual distributions and comparing to Bayesian MAP estimates)\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### <center> Questions?\n",
    "___\n",
    "    \n",
    "<center><img src=\"figures/dt160107.gif\" width=70%>"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "482.913px",
    "left": "126px",
    "top": "174.923px",
    "width": "494px"
   },
   "toc_section_display": false,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
