{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2019 Steven Mattis and Troy Butler\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import dynamical_systems as ds\n",
    "from scipy.stats import gaussian_kde as GKDE\n",
    "from math import *\n",
    "from luq import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is the Sel'kov model for glycolysis, a process by which living cells breakdown sugar to obtain energy:\n",
    "$$x' = -(x+b) + a \\left(y + \\frac{b}{a+b^2} \\right) + (x+b)^2 \\left(y + \\frac{b}{a+b^2}\\right)$$\n",
    "$$y' = b-a\\left(y+ \\frac{b}{a+b^2}\\right) - (x+b)^2 \\left(y + \\frac{b}{a+b^2}\\right), $$\n",
    "where $x$ and $y$ represent concentrations of ADP and F6P, respectively, and $a,b>0$.\n",
    "The initial conditions are $x(0) = x_0 \\in \\mathbb{R}$ and $y(0) = y_0 \\in \\mathbb{R}$.\n",
    "\n",
    "The system has Hopf Bifurcations at \n",
    "$$b = b_1(a) = \\sqrt{(1-\\sqrt{1-8a}-2a)/2}$$\n",
    "and\n",
    "$$b = b_2(a) = \\sqrt{(1+\\sqrt{1-8a}-2a)/2}.$$\n",
    "If $b<b_1$, the origin is a stable focus. If $b_1 < b < b_2$, there is a stable periodic orbit.\n",
    "If $b > b_2$ the origin is a stable focus.\n",
    "\n",
    "The system is solved numerically using the RK45 method.\n",
    "\n",
    "A ***true*** distribution of $a, b,  x_0$, and $y_0$ are defined by (non-uniform)\n",
    "Beta distributions and used to generate a set of time series data.\n",
    "\n",
    "An ***initial*** uniform distribution is assumed and updated by the true time series data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uniformly sample the parameter samples to form a \"prediction\" or \"test\" set\n",
    "num_samples = int(1E3)\n",
    "\n",
    "param_range = np.array([[0.01, 0.12], # a\n",
    "                       [0.05, 1.5]])  #b\n",
    "ic_range = np.array([[0.5, 2.0],  # y_0\n",
    "                     [0.5, 2.0]])  # x_0\n",
    "\n",
    "params = np.random.uniform(size=(num_samples, 2))\n",
    "params = param_range[:, 0] + (param_range[:, 1] - param_range[:, 0]) * params\n",
    "\n",
    "ics = np.random.uniform(size=(num_samples, 2))\n",
    "ics = ic_range[:, 0] + (ic_range[:, 1] - ic_range[:, 0]) * ics\n",
    "\n",
    "# labels\n",
    "param_labels = [r'$a$', r'$b$']\n",
    "ic_labels = [r'$x_0$', r'$y_0$']\n",
    "\n",
    "# Construct the predicted time series data\n",
    "num_time_preds = int(1000)  # number of predictions (uniformly space) between [time_start,time_end]\n",
    "time_start = 0.5\n",
    "time_end = 40.0\n",
    "times = np.linspace(time_start, time_end, num_time_preds)\n",
    "\n",
    "# Solve systems\n",
    "phys = ds.Selkov()\n",
    "predicted_time_series = phys.solve(ics=ics, params=params, t_eval=times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate an observed Beta distribution of time series data\n",
    "\n",
    "num_obs = int(1E3)\n",
    "\n",
    "true_a = 2\n",
    "true_b = 2\n",
    "\n",
    "params_obs = np.random.beta(size=(num_obs, 2), a=true_a, b=true_b)\n",
    "params_obs = param_range[:, 0] + (param_range[:, 1] - param_range[:, 0]) * params_obs\n",
    "\n",
    "ics_obs = np.random.beta(size=(num_obs, 2), a=true_a, b=true_b)\n",
    "ics_obs = ic_range[:, 0] + (ic_range[:, 1] - ic_range[:, 0]) * ics_obs\n",
    "\n",
    "# Solve system\n",
    "observed_time_series = phys.solve(ics=ics_obs, params=params_obs, t_eval=times)\n",
    "\n",
    "# Add noise if desired\n",
    "with_noise = False\n",
    "noise_stdev = 0.05\n",
    "\n",
    "if with_noise:\n",
    "    observed_time_series += noise_stdev * np.random.randn(num_obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use LUQ to learn dynamics and QoIs\n",
    "learn = LUQ(predicted_time_series, observed_time_series, times)\n",
    "\n",
    "# time array indices over which to use\n",
    "time_start_idx = 500\n",
    "time_end_idx = 799\n",
    "\n",
    "# Clean data\n",
    "learn.clean_data(time_start_idx=time_start_idx, time_end_idx=time_end_idx,\n",
    "                 num_clean_obs=100, tol=3.0e-2, min_knots=15, max_knots=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learn and classify dynamics\n",
    "learn.dynamics(cluster_method='kmeans', kwargs={'n_clusters': 2, 'n_init': 10})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot clusters of predicted time series\n",
    "for j in range(learn.num_clusters):\n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(10,2.5), \n",
    "                                        gridspec_kw={'width_ratios': [3,1,1]}) \n",
    "    ps = []\n",
    "    ps2 = []\n",
    "    for i in range(num_samples):\n",
    "        if learn.predict_labels[i] == j:\n",
    "            a = params[i, 0]\n",
    "            b = params[i, 1]\n",
    "            b1 = sqrt((1.0-sqrt(1.-8.*a)-2.*a)/2.0)\n",
    "            b2 = sqrt((1.0+sqrt(1.-8.*a)-2.*a)/2.0)\n",
    "            ps.append(b/b1)\n",
    "            ps2.append(b/b2)\n",
    "            ax1.plot(learn.clean_times, learn.clean_predictions[i, :])\n",
    "    ax1.set(title='Cluster ' + str(j))\n",
    "    xs = np.linspace(0.0, 5.0, 100)\n",
    "    ax2.plot(xs, GKDE(ps)(xs))\n",
    "    ax2.axvline(1.0, color='r')\n",
    "    ax2.set(xlabel=r'$b/b_1$', title='Param. Distrib.')\n",
    "    xs2 = np.linspace(0.0, 2.0, 100)\n",
    "    ax3.plot(xs2, GKDE(ps2)(xs2))\n",
    "    ax3.axvline(1.0, color='r')\n",
    "    ax3.set(xlabel=r'$b/b_2$', title='Param. Distrib.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot oberved and predicted clusters\n",
    "for j in range(learn.num_clusters):\n",
    "    plt.figure()\n",
    "    cluster_num = j\n",
    "    for i in range(num_samples):\n",
    "        if learn.predict_labels[i] == cluster_num:\n",
    "            plt.plot(learn.clean_times, learn.clean_predictions[i,:],'b*')\n",
    "    for i in range(num_obs):\n",
    "        if learn.obs_labels[i] == cluster_num:\n",
    "            plt.plot(learn.clean_times, learn.clean_obs[i,:],'ro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find best KPCA transformation for given number of QoI and transform time series data.\n",
    "predict_map, obs_map = learn.learn_qois_and_transform(num_qoi=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate kernel density estimates on new QoI\n",
    "learn.generate_kdes()\n",
    "# Calculate rejection rates for each cluster and print averages.\n",
    "r_vals = learn.compute_r()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute marginal probablities for each parameter and initial condition.\n",
    "param_marginals = []\n",
    "ic_marginals = []\n",
    "true_param_marginals = []\n",
    "true_ic_marginals = []\n",
    "lam_ptr = []\n",
    "cluster_weights = []\n",
    "for i in range(learn.num_clusters):\n",
    "    lam_ptr.append(np.where(learn.predict_labels == i)[0])\n",
    "    cluster_weights.append(len(np.where(learn.obs_labels == i)[0]) / num_obs)\n",
    "\n",
    "for i in range(params.shape[1]):\n",
    "    true_param_marginals.append(GKDE(params_obs[:,i]))\n",
    "    param_marginals.append([])\n",
    "    for j in range(learn.num_clusters):\n",
    "        param_marginals[i].append(GKDE(params[lam_ptr[j], i], weights=learn.r[j]))\n",
    "        \n",
    "for i in range(ics.shape[1]):\n",
    "    true_ic_marginals.append(GKDE(ics_obs[:,i]))\n",
    "    ic_marginals.append([])\n",
    "    for j in range(learn.num_clusters):\n",
    "        ic_marginals[i].append(GKDE(ics[lam_ptr[j], i], weights=learn.r[j]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uniform distribution\n",
    "def unif_dist(x, p_range):\n",
    "    y = np.zeros(x.shape)\n",
    "    val = 1.0/(p_range[1] - p_range[0])\n",
    "    for i, xi in enumerate(x):\n",
    "        if xi < p_range[0] or xi >  p_range[1]:\n",
    "            y[i] = 0\n",
    "        else:\n",
    "            y[i] = val\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot predicted marginal densities for parameters\n",
    "\n",
    "for i in range(params.shape[1]):\n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "    fig.clear()\n",
    "    x_min = min(min(params[:, i]), min(params_obs[:, i]))\n",
    "    x_max = max(max(params[:, i]), max(params_obs[:, i]))\n",
    "    delt = 0.25*(x_max - x_min)\n",
    "    x = np.linspace(x_min-delt, x_max+delt, 100)\n",
    "    plt.plot(x, unif_dist(x, param_range[i, :]),\n",
    "         label = 'Initial guess')\n",
    "    mar = np.zeros(x.shape)\n",
    "    for j in range(learn.num_clusters):\n",
    "        mar += param_marginals[i][j](x) * cluster_weights[j]\n",
    "    plt.plot(x, mar, label = 'Estimated pullback')\n",
    "    plt.plot(x, true_param_marginals[i](x), label = 'Actual density')\n",
    "    plt.title('Comparing pullback to actual density of parameter ' + param_labels[i], fontsize=16)\n",
    "    plt.legend(fontsize=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot predicted marginal densities for initial conditions.\n",
    "\n",
    "for i in range(ics.shape[1]):\n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "    fig.clear()\n",
    "    x_min = min(min(ics[:, i]), min(ics_obs[:, i]))\n",
    "    x_max = max(max(ics[:, i]), max(ics_obs[:, i]))\n",
    "    delt = 0.25*(x_max - x_min)\n",
    "    x = np.linspace(x_min-delt, x_max+delt, 100)\n",
    "    plt.plot(x, unif_dist(x, ic_range[i, :]),\n",
    "         label = 'Initial guess')\n",
    "    mar = np.zeros(x.shape)\n",
    "    for j in range(learn.num_clusters):\n",
    "        mar += ic_marginals[i][j](x) * cluster_weights[j]\n",
    "    plt.plot(x, mar, label = 'Estimated pullback')\n",
    "    plt.plot(x, true_ic_marginals[i](x), label = 'Actual density')\n",
    "    plt.title('Comparing pullback to actual density of initial condition ' + ic_labels[i], fontsize=16)\n",
    "    plt.legend(fontsize=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
